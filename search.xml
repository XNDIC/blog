<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>NERF</title>
      <link href="/2024/10/16/NERF/"/>
      <url>/2024/10/16/NERF/</url>
      
        <content type="html"><![CDATA[<h1 id="NERF"><a href="#NERF" class="headerlink" title="NERF"></a>NERF</h1><p>学习来源：<a href="https://www.bilibili.com/video/BV1CC411V7oq/?spm_id_from=333.999.0.0&amp;vd_source=2c97cd7eec4f38788b5479fea364bd65">【较真系列】讲人话-NeRF全解（原理+代码+公式）</a></p><h3 id="什么是NeRF"><a href="#什么是NeRF" class="headerlink" title="什么是NeRF?"></a>什么是NeRF?</h3><h5 id="广义理解定义："><a href="#广义理解定义：" class="headerlink" title="广义理解定义："></a>广义理解定义：</h5><ul><li><p>使用神经网络(MLP)来<strong>隐式的</strong>存储3D信息。</p><ul><li>显式的3D信息：有明确x，y，z的值(mesh，voxel，点云…)<ul><li>mesh：明确的xyz矩阵+边的关系</li></ul></li><li>隐式的3D信息：无明确的x/y/z的值，只能输出指定角度的2D图片。</li></ul></li><li><p>训练时使用给定静止场景下的若干张图片</p><p><img src="/2024/10/16/NERF/image-20240924091335377.png" alt="image-20240924091335377" style="zoom:80%;"></p></li></ul><h5 id="推论："><a href="#推论：" class="headerlink" title="推论："></a>推论：</h5><ul><li>模型不具有泛化能力</li><li>一个模型只能存储一个3D信息</li></ul><p><a href="https://arxiv.org/pdf/2003.08934">论文：</a></p><ol><li>模型输入是5D向量→粒子的空间位姿(x, y, z, theta, phi)</li><li>模型输出是4D向量→粒子对应的颜色以及密度(密度，颜色RGB)</li><li>模型是8层的MLP</li></ol><p>模型前处理：图像转5D向量</p><p>模型后处理：4D向量转2D图片</p><h3 id="真实场景及相机模型"><a href="#真实场景及相机模型" class="headerlink" title="真实场景及相机模型"></a>真实场景及相机模型</h3><p>真实场景：有多个光源+不同物体间的折射/反射</p><p><img src="/2024/10/16/NERF/image-20240924094319157.png" alt="image-20240924094319157" style="zoom:80%;"><img src="/2024/10/16/NERF/image-20240924094415221.png" alt="image-20240924094415221" style="zoom:80%;"></p><p>相机模型：连接3d世界与2d图片</p><ul><li>世界坐标系</li><li>相机坐标系</li><li>归一化相机坐标系（物理成像平面CCD）</li><li>像素坐标系（UV坐标）</li></ul><p>补充：<a href="https://blog.csdn.net/MengYa_Dream/article/details/120233806">图像处理：4个坐标系及相关转换</a></p><h4 id="体渲染"><a href="#体渲染" class="headerlink" title="体渲染"></a>体渲染</h4><p>定义：</p><ul><li>属于渲染技术的分支，目的是解决云/烟/果冻等非刚性物体的渲染建模</li><li>将物质抽象成一团飘忽不定的粒子群</li><li>光线在穿过时，是光子在跟粒子发生碰撞的过程。</li></ul><p>光子与粒子发生作用的过程：</p><ul><li>吸收：光子被粒子吸收</li><li>放射：粒子本身发光</li><li>外射光：光泽在冲击后，被弹射</li><li>内射光：其他方向弹射来的粒子</li></ul><h5 id="NeRF假设："><a href="#NeRF假设：" class="headerlink" title="NeRF假设："></a>NeRF假设：</h5><ul><li>物体是一团自发光的粒子，粒子有密度和颜色</li><li>外射光和内射光抵消</li><li>多个自发光的粒子被渲染成指定角度的图片</li></ul><h3 id="NeRF的输入和输出"><a href="#NeRF的输入和输出" class="headerlink" title="NeRF的输入和输出"></a>NeRF的输入和输出</h3><p>模型的输入：将物体进行稀疏表示的<strong>单个粒子的位姿</strong></p><p>模型的输出：<strong>该粒子的密度和颜色</strong></p><p><img src="/2024/10/16/NERF/image-20240924095330940.png" alt="image-20240924095330940" style="zoom: 60%;"></p><p>问题：</p><ul><li>图片呢？</li><li>怎么得到这些粒子？</li><li>多少个粒子?这些粒子怎么批量输入？</li><li>这些粒子是怎么渲染成新的图片的？</li></ul><h3 id="粒子的采集"><a href="#粒子的采集" class="headerlink" title="粒子的采集"></a>粒子的采集</h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>对于空间中的某一个自发光粒子：假设空间坐标(x,y,z)，发射的光线通过相机模型成为图片上的像素坐标(u,v)，粒子颜色即为像素颜色。</p><p>(u,v)与(x,y,z)的公式如下：</p><p><img src="/2024/10/16/NERF/image-20240924095751794.png" alt="image-20240924095751794" style="zoom:70%;"></p><p>反之，对于图片上的某一个像素(u,v)的颜色，可以看作是<strong>沿着某一条射线上的无数个发光点的“和”</strong>。</p><p>利用相机模型，反推射线，那么这个射线表示为：$r(t)=o+td$，$o$为射线原点，$d$为方向，$t$为距离，理论上t是从0到$+∞$；用极坐标表示。</p><p><img src="/2024/10/16/NERF/image-20240924100410260.png" alt="image-20240924100410260"></p><p>如果一张照片是H×W的大小，对于整张图片共有(H,W)条射线。</p><h4 id="光线原理"><a href="#光线原理" class="headerlink" title="光线原理"></a>光线原理</h4><p>由像素点P(u, v)反推射线：</p><ul><li>像素平面→物理成像平面：$(x_n, y_n)=(-(u-\frac{w}{2}),v-\frac{h}{2})$</li><li>物理成像平面→相机坐标系：$(x_c, y_c, z_c)=(x_n, y_n, -f)$，其中f为焦距</li><li>归一化：$(x_c, y_c, z_c)=(\frac{x_c}{f}, \frac{y_c}{f}, -1)$</li><li>相机坐标系→世界坐标系：$(x_w, y_w, z_w)=c2w*(x_c, y_c, z_c)$，其中c2w指camera to world(RT旋转矩阵)</li></ul><p><img src="/2024/10/16/NERF/image-20241010204145148.png" alt="image-20241010204145148"></p><h4 id="光线代码pytorch-nerf"><a href="#光线代码pytorch-nerf" class="headerlink" title="光线代码pytorch_nerf"></a>光线代码pytorch_nerf</h4><p><img src="/2024/10/16/NERF/image-20241010205316190.png" alt="image-20241010205316190"></p><ul><li>输入：h*w，k相机内参，c2w旋转矩阵+平移矩阵</li><li>i,j：像素的表达，eg: h=400,w=400，i,j表示(0,0)(0,1)…</li><li>dirs：directions，归一化的相机坐标</li><li>rays_d：dirs<em>c2w，射线的<em>*方向</em></em>，世界坐标系</li><li>rays_o：世界坐标系的原点，相机坐标原点*c2w</li><li>输出：rays_o,rays_d，射线的表达</li></ul><p>因为图像的射线有h×w个，过多，要进行筛选，所以引入batch_size，一般为1024。因为原点的表示为[x0,y0,z0]，图像上的一点为[x1,y1,z1]，所以距离为[x0-x1,y0-y1,z0-z1]，即三维[1024,3]。采样是random随机采集像素。采样一般有两种，一种是一张图片选1024个，一种是所有图片中选1024个，每个射线*各自c2w以解决不同的位姿。</p><h4 id="粒子采样原理"><a href="#粒子采样原理" class="headerlink" title="粒子采样原理"></a>粒子采样原理</h4><p>对于图片上的某一个像素(u,v)：沿着某一条射线上的无数个发光点的“和”，射线表示为：$r(t)=o+td$，其中o为射线原点，d为方向，t为距离。理论上t从0到+∞。</p><p>但在计算中t是离散的，所以选择t的方法如下：设置near=2,far=6，在near和far之间均匀采样64个点，得到$t_i=U[t_n+\frac{i-1}{N}(t_f-t_n), t_n+\frac{i}{N}(t_f-t_n)]$</p><p>在2-6中均匀采样64个点并加一些噪声，加一些扰动（效果更好，对feature的噪声更加鲁棒）<br>Robustness：承受故障和干扰的能力，一些异常的数据对整体性能的影响不大。</p><h4 id="粒子采样代码"><a href="#粒子采样代码" class="headerlink" title="粒子采样代码"></a>粒子采样代码</h4><p><img src="/2024/10/16/NERF/image-20241010210044743.png" alt="image-20241010210044743"></p><ul><li><p>near和far：size是(1024,1)，near每个数字都是2，far每个数字都是6</p></li><li><p>linespace：从0-1中采样64个</p></li><li><p>一共是1024*64个点，shape是(1024,64,3)</p></li></ul><h3 id="回答关于输入的问题"><a href="#回答关于输入的问题" class="headerlink" title="回答关于输入的问题"></a>回答关于输入的问题</h3><ol><li>图片呢?怎么得到这些粒子?<br>从图片和相机位姿计算射线，从射线上采样粒子</li><li>多少个粒子?这些粒子怎么批量输入?<br>训练时，一张图片取1024个像素，得到1024条射线，每条射线上采样64个粒子，共1024*64个粒子，粒子以batch形式输入模型</li><li>如何从2D的UV变成5D的输入？<br>以batch形式输入，[1024*64,3]，其中3代表[x,y,z]<br>输入是x,y, z, theta, phi，theta和phi在正常3D表示中分别代表了仰角和方位角，在代码里通过rays_d表示</li></ol><h3 id="输出是什么？"><a href="#输出是什么？" class="headerlink" title="输出是什么？"></a>输出是什么？</h3><p>模型的输出：该粒子的密度和颜色</p><p><img src="/2024/10/16/NERF/image-20241010210440705.png" alt="image-20241010210440705"></p><ul><li>pts：1024<em>64</em>3，即xyz</li><li>viewdirs：方向向量rays_d</li><li>network_fn：MLP多层感知器</li></ul><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>8层全连接层，半路再次输入位置坐标，后半路输出密度σ，后半路输入方向视角，最后输出颜色RGB</p><p><img src="/2024/10/16/NERF/image-20241010211442328.png" alt="image-20241010211442328"></p><h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p><img src="/2024/10/16/NERF/image-20241010210917779.png" alt="image-20241010210917779"></p><p>如果只输入3D的xyz和3D的theta,phi，会丢失细节。所以引入位置编码，即加入sin和cos，和transformer一样</p><ul><li>对于空间坐标x，即xyz，对于每一维度都计算sin和cos，L=10即在0-10进行计算，有20个数，三维加在一起是60维</li><li>对于视角坐标d，即rays_d，L=4即在0-4进行计算，有8个数，三维加在一起是24维</li><li>最后加上原始的三个维度，得到xyz是63维，d是27维【初始值是代表原点？】</li></ul><p>L的值不是固定不变的，可能作者觉得粒子的坐标比方向更重要，所以如此取值。因为模型先通过坐标得到了密度的输出后，在加上direction的信息得到颜色（观测方向不应当影响密度，但会改变颜色）</p><h3 id="模型结构代码"><a href="#模型结构代码" class="headerlink" title="模型结构代码"></a>模型结构代码</h3><p><img src="/2024/10/16/NERF/image-20241010211137004.png" alt="image-20241010211137004"></p><p>整体就是全连接层FC的叠加。在第5层时再次输入位置坐标，即256+63=319；在倒数第二层加入了direction的信息，即256+27=283</p><h3 id="Loss的计算"><a href="#Loss的计算" class="headerlink" title="Loss的计算"></a>Loss的计算</h3><p>采用自监督：GT是图片某一像素的RGB，将该像素对应光线上的粒子颜色进行求和</p><p>粒子的颜色和：该像素颜色的预测值</p><p>粒子的颜色和（真实GT的颜色）与像素颜色（预测的颜色）做MSE：$L=\sum_{r∈R}||\hat{C}(r)-C(r)||_2^2$，其中r是射线，R是每个batch的射线(1024条)</p><h3 id="体渲染-1"><a href="#体渲染-1" class="headerlink" title="体渲染"></a>体渲染</h3><p>如何将光线上粒子颜色进行求和？</p><p>T(s)：在s点之前，光线没有被阻碍的概率；如果光线被阻碍，T(s)=0，则无后续颜色</p><p>σ(s)：在s点处，光线碰击粒子(光线被粒子阻碍)的概率密度</p><p>C(s)：在s点处，粒子光出的颜色</p><p>σ(s)和C(s)是模型的输出，各点的颜色和概率密度已知，先求T(s)</p><p>$\hat{C(r)}=\int_0^{+\infty}T(s)\sigma(s)C(s)ds$</p><p>$T(s)=e^{-\int_0^s\sigma(t)dt}$</p><p><img src="/2024/10/16/NERF/image-20241016135927932.png" alt="image-20241016135927932" style="zoom: 80%;"><img src="/2024/10/16/NERF/image-20241016140605711.png" alt="image-20241016140605711" style="zoom:80%;"></p><h4 id="T-s-的推导"><a href="#T-s-的推导" class="headerlink" title="T(s)的推导"></a>T(s)的推导</h4><p>σ表示密度，密度越大，不被阻拦的概率越小，即1-σ(s)ds</p><p>T(0)在原点不被阻障的概率，T(0)=1，lnT(0)=0</p><p>$T(s+ds)=T(s)[1-\sigma(s)ds]=T(s)-T(s)\sigma(s)ds$</p><p>$T(s+ds)-T(s)=-T(s)\sigma(s)ds$</p><p>$dT(s)=-T(s)\sigma(s)ds$</p><p>$ \frac{dT(s)}{T(s)}=-\sigma(s)ds$</p><p>$\int_0^t-\sigma(s)ds=\int_0^t\frac{dT(s)}{T(s)}=\int_0^t\frac{1}{T(s)}dT(s)=\ln T(s)\vert_0^t=\ln T(t)-\ln T(0)=\ln T(t)$</p><p>$T(t)=e^{-\int_0^s\sigma(t)dt}$</p><h4 id="离散积分"><a href="#离散积分" class="headerlink" title="离散积分"></a>离散积分</h4><p>计算机只能处理离散化数据</p><p>将光线[0,s]划分为N个等间距区间[$T<em>n$→$T</em>{n+1}$]，其中n=0,1,2,…,N；间隔长度为$\delta_n$，假设区间内密度$\sigma_n$和颜色$C_n$固定</p><p>$\hat{C(r)}=\sum<em>{i=1}^NT_i(1-e^{-\sigma_i\delta_i})C_i$，where $T_i=e^{-\sum</em>{j=1}^{i-1}\sigma_j\delta_j}$</p><h5 id="离散积分推导"><a href="#离散积分推导" class="headerlink" title="离散积分推导"></a>离散积分推导</h5><p>每个光区贡献光强进行累加$\hat{C}=\sum<em>{n=0}^NI(T_n\rightarrow T</em>{n+1})$，其中s是沿光流方向的光线的长度，I(s)是距离s处的光强度</p><script type="math/tex; mode=display">\begin{align*}I(T_n\rightarrow T_{n+1}) &= \int_{t_n}^{t_{n+1}}T(t)\sigma(n)C(n)dt \\&= \sigma(n)C(n)\int_{t_n}^{t_{n+1}}T(t)dt \\&= \sigma(n)C(n)\int_{t_n}^{t_{n+1}}e^{-\int_0^t\sigma(s)ds}dt\\&= \sigma(n)C(n)\int_{t_n}^{t_{n+1}}e^{-(\int_0^{t_n}\sigma(s)ds+\int_{t_n}^t\sigma(s)ds)}dt\\&= \sigma(n)C(n)\int_{t_n}^{t_{n+1}}e^{-\int_0^{t_n}\sigma(s)ds}e^{-\int_{t_n}^t\sigma(s)ds}dt\\&= \sigma(n)C(n)T(0\rightarrow t_n)\int_{t_n}^{t_{n+1}}e^{-\int_{t_n}^t\sigma(s)ds}dt\\&= \sigma(n)C(n)T(0\rightarrow t_n)\int_{t_n}^{t_{n+1}}e^{-\int_{t_n}^t\sigma_nds}dt\\&= \sigma(n)C(n)T(0\rightarrow t_n)\int_{t_n}^{t_{n+1}}e^{-\sigma_n(t-t_n)}dt【注：\sigma(n)是常数，做积分原函数是x，故得到t-t_n】\\&= \sigma(n)C(n)T(0\rightarrow t_n)[-\frac1{\sigma_n}e^{-\sigma_n(t-t_n)}\vert_{t_n}^{t_{n+1}}]\\&= \sigma(n)C(n)T(0\rightarrow t_n)(-\frac1{\sigma_n}(e^{-\sigma_n\delta_n}-1))【注：e^{-\sigma_n(t-t_n)}\vert_{t_n}^{t_{n+1}} = e^{-\sigma_n(t_{n+1}-t_n)}-e^{-\sigma_n(t_n-t_n)} = e^{-\sigma_n\delta_n}-1】\\&= C(n)T(0\rightarrow t_n)(-e^{-\sigma_n\delta_n}+1)【注：约去\sigma(n)】\\&= C(n)e^{-\sum_{i=0}^{n-1}\sigma_i\delta_i}(-e^{-\sigma_n\delta_n}+1)【注：T(0\rightarrow t_n)=e^{-\int_0^{t_n}\sigma(s)ds}=e^{-\sum_{i=0}^{n-1}\sigma_i\delta_i}】\\\end{align*}</script><p>$\hat{C}=\sum<em>{n=0}^NI(T_n\rightarrow T</em>{n+1})=\sum<em>{n=0}^NC_ne^{-\sum</em>{i=0}^{n-1}\sigma_i\delta_i}(-e^{1-\sigma_n\delta_n})$</p><p>设$\alpha_n=1-e^{-\sigma_n\delta_n}$，即$e^{-\sigma_n\delta_n}=1-a_n$可以看作第n个点的不透明度</p><script type="math/tex; mode=display">\begin{align*}e^{-\sum_{i=0}^{n-1}\sigma_i\delta_i} &= e^{-(\sigma_0\delta_0+\sigma_1\delta_1+...+\sigma_{n-1}\delta_{n-1})}\\&= e^{-\sigma_0\delta_0}e^{-\sigma_1\delta_1}...e^{-\sigma_{n-1}\delta_{n-1}}\\&= (1-a_0)(1-a_1)...(1-a_{n-1})\\故\hat{C}&=\sum_{n=0}^NC_na_n(1-a_0)(1-a_1)...(1-a_{n-1})\\&=C_0a_0+C_1a_1(1-a_0)+C_2a_2(1-a_0)(1-a_1)+...+C_na_n(1-a_0)(1-a_1)...(1-a_{n-1})\end{align*}</script><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><p>delta_n：粒子取样的间隔，相同的数，大致都是0.06</p><p><img src="/2024/10/16/NERF/image-20241016155640007.png" alt="image-20241016155640007"></p><p>一个射线对应一个像素，最后得到1024*3，3指rgb</p><h3 id="回答关于输出的问题"><a href="#回答关于输出的问题" class="headerlink" title="回答关于输出的问题"></a>回答关于输出的问题</h3><p>这些粒子是怎么渲染成新的图片的？<br>分别计算图片中每一个像素的颜色，计算该像素对应的光线和粒子，将这些粒子通过公式累加，得到该像素最终颜色。</p><h3 id="二次采样"><a href="#二次采样" class="headerlink" title="二次采样"></a>二次采样</h3><p>目前有效区域和无效区域(空白区域和遮挡区域)都是均匀采样</p><p>我们希望：有效区域多采样，无效区域少采样或不采样</p><p>解决方法：可以根据概率密度进行再次采样</p><p><img src="/2024/10/16/NERF/image-20241016160028491.png" alt="image-20241016160028491"></p><h4 id="最终模型结构"><a href="#最终模型结构" class="headerlink" title="最终模型结构"></a>最终模型结构</h4><p>两个8层的MLP串联得到最终模型，第一个MLP为粗模型（均匀采样64个点，得到点的概率密度函数），第二个MLP叫细模型（根据一个模型的概率密度函数，二次采样更有价值点，再进行粒子的求和，得到最终的像素颜色值）</p><p>粗模型和细模型结构相同，最后输出为模型2的输出</p><h4 id="逆变换采样"><a href="#逆变换采样" class="headerlink" title="逆变换采样"></a>逆变换采样</h4><p>细模型根据密度进行二次采样的方法：</p><ul><li>根据粗模型的结果，进行逆变换采样。</li><li>对于每条光线，重新采样128个粒子，与之前的64个粒子加在一起，即每条光线采样192个粒子。</li><li>对于每条射线上的粒子颜色前的权重，即$\alpha<em>n(1-\alpha_0)(1-\alpha_1)…(1-\alpha</em>{n-1})$做softmax，即$\hat{w<em>j}=\frac{w_i}{\sum</em>{j=1}^{N_c}w_i}$，此时，新的权重和为1，可看作PDF(概率密度函数)</li><li>生成它的cdf(累积分布函数，概率密度函数的积分)</li><li>invert cdf(反函数)</li><li>用均匀分布drand480生成一个随机数：invert(drand48)=r，得到的r就是符合pdf分布的随机数</li></ul><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><p><img src="/2024/10/16/NERF/image-20241016161034828.png" alt="image-20241016161034828"></p><p>最后输出(192,3)维度的颜色数据</p><h3 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h3><p>输入：400×400条射线上分别采样64个点</p><p>输出：[400×400×192,4]，其中[h×w×192,4]中4指rgb和σ；进行体渲染</p><p><img src="/2024/10/16/NERF/image-20241016161301952.png" alt="image-20241016161301952"></p><h3 id="Loss是怎样计算的"><a href="#Loss是怎样计算的" class="headerlink" title="Loss是怎样计算的?"></a>Loss是怎样计算的?</h3><p><img src="https://upload-images.jianshu.io/upload_images/12824314-33db12476e16fd6a.png?imageMogr2/auto-orient/strip|imageView2/2/w/1142/format/webp" alt="img"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>前处理：</p><ul><li>将图片中的每个像素，通过相机模型找到对应的射线</li><li>在每条射线上进行采样，得到64个粒子</li><li>对batch_size*64个粒子进行位置编码</li><li>位置坐标为63D和方向向量为27D</li></ul><p>模型1：</p><ul><li>8层MLP</li><li>输入为(batch_size,64,63)和(batch_size,64,27)</li><li>输出为(batch_size,64,4)</li></ul><p>后处理1：</p><ul><li>计算模型1的输出，对射线进行二次采样</li><li>每条射线上共采样192个粒子</li></ul><p>模型2：</p><ul><li>8层MLP</li><li>输入为(batch_size,192,63)和(batch_size,192,27)</li><li>输出为(batch_size,192,4)</li></ul><p>后处理2：将模型2输出通过体渲染，转换为像素。</p><p>核心内容：体渲染、位置编码、层级采样</p><p>缺点：很慢，训练/推理都慢；只能表达静态场景；对光照处理的一般；没有泛化能力。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 数字人 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>吴恩达2022机器学习</title>
      <link href="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
      <url>/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="课程来源"><a href="#课程来源" class="headerlink" title="课程来源"></a>课程来源</h2><p><a href="https://www.bilibili.com/video/BV1Zt4y1H78P/?share_source=copy_web&amp;vd_source=de6d7d10ceabb39ea60e61fa8c108937">吴恩达2022机器学习专项课程(一）监督学习 Supervised Learning</a></p><p><a href="https://www.bilibili.com/video/BV1nt4y1h7jc/?share_source=copy_web&amp;vd_source=de6d7d10ceabb39ea60e61fa8c108937">吴恩达2022机器学习专项课程(二）：高级学习算法 Advanced Learning Algorithms</a></p><p><a href="https://www.bilibili.com/video/BV1ja411S7Wq/?share_source=copy_web&amp;vd_source=de6d7d10ceabb39ea60e61fa8c108937">吴恩达2022机器学习专项课程(三）：无监督学习/推荐系统/强化学习 Unsupervised Learning/Recommenders/Reinforcement</a></p><h2 id="分类算法Classification"><a href="#分类算法Classification" class="headerlink" title="分类算法Classification"></a>分类算法Classification</h2><p>二分类问题(binary classification)：结果只有两种可能的类(classes)，或两种可能的类别(categories)</p><p>术语：正样本(positive class)：true/1；负样本(negative class)：false/0</p><p>线性回归会因为添加训练样本而改变预测结论，如下图所示：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804212737004.png" alt="image-20240804212737004" style="zoom:50%;"></p><h3 id="逻辑回归算法Logistic-Regression"><a href="#逻辑回归算法Logistic-Regression" class="headerlink" title="逻辑回归算法Logistic Regression"></a>逻辑回归算法Logistic Regression</h3><p>逻辑回归方程：$f_\vec{w},_b(\vec{x})=g(\vec{w}\cdot\vec{x}+b)=\frac{1}{1+e^{-(\vec{w}\cdot\vec{x}+b)}}$</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804213348751.png" alt="image-20240804213348751" style="zoom:50%;"></p><p>可以把输出看作是在给定输入x的情况下，类别或标签y等于1的概率。</p><p>$f_\vec{w},_b(\vec{x})=P(y=1|\vec{x},\vec{w},b)$：表明在给定输入特征w和b（参数w和b影响计算）的前提下，y=1的概率是多少（条件概率）</p><p>设置一个阈值(threshold) ，超过这个阈值则预测y=1，通常阈值被设置为0.5。</p><p>通过阈值预测结果的原理如下图蓝框内容：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804214826761.png" alt="image-20240804214826761" style="zoom:50%;"></p><h3 id="决策边界Decision-Boundary"><a href="#决策边界Decision-Boundary" class="headerlink" title="决策边界Decision Boundary"></a>决策边界Decision Boundary</h3><p>线性和非线性决策边界示例如下：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804215353490.png" alt="image-20240804215353490" style="zoom:50%;"><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804215430301.png" alt="image-20240804215430301" style="zoom:50%;"></p><p>通过多项式特征，可以得到非常复杂的决策边界。换包话说，逻辑回归可以学会拟合相当复杂的数据。</p><p>如果不使用高阶多项式，那么逻辑回归的决策边界永远是线性的。</p><h3 id="代价函数Cost-Function"><a href="#代价函数Cost-Function" class="headerlink" title="代价函数Cost Function"></a>代价函数Cost Function</h3><p>逻辑回归函数的代价函数是非凸的(non-convex)，这意味着如果想用梯度下降法，因为有很多局部极小值，所以很容易卡在这些地方。<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804220308650.png" alt="image-20240804220308650" style="zoom:50%;"><br>所以我们需要让代价函数再次凸化，保证梯度下降可以收敛道全局最小值。如上图$J(\vec{w},b)$（1/2被挪到了累加公式里）。</p><p>损失函数衡量的是在一个训练样本中的表现，把所有的训练样本的损失加起来得到的代价函数，才能衡量模型在整个训练集上的表现。</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804221342091.png" alt="image-20240804221342091" style="zoom:50%;"><br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804221614563.png" alt="image-20240804221614563" style="zoom:50%;"></p><p>为了便于写代码，可以将损失函数L简化，简化后的损失函数(loss function)和代价函数(cost function)如下所示：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804222203792.png" alt="image-20240804222203792" style="zoom:50%;"></p><p><a href="https://blog.csdn.net/qq_41775769/article/details/113514294">一文彻底读懂【极大似然估计】-CSDN博客</a></p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>==导数结果不懂，前面整理完后回顾==</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804223110637.png" alt="image-20240804223110637" style="zoom:50%;"></p><p><a href="https://www.cnblogs.com/zhongmiaozhimen/p/6155093.html">第三周：逻辑回归代价函数求导过程 - 玄天妙地 - 博客园 (cnblogs.com)</a></p><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>欠拟合underfit、高偏差high bias、泛化generalization、过拟合overfit、高方差high variance<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804224609969.png" alt="image-20240804224609969" style="zoom:50%;"></p><p>解决过拟合的方法：①增加训练样本②使用更少的特征（特征选择feature selection）③正则化regularization</p><h3 id="正则化Regularization"><a href="#正则化Regularization" class="headerlink" title="正则化Regularization"></a>正则化Regularization</h3><p>正则化是尽可能地让算法缩小参数的值，而不是一定要求参数为0。</p><p>当模型参数很多时，我们不清楚哪些参数是最重要的，就会对所有参数进行惩罚，即把所有参数都缩小点。<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804225912759.png" alt="image-20240804225912759" style="zoom:50%;"><br>上式中<script type="math/tex">\lambda</script>表示正则化参数；累加函数前都除以2m，是为了用同样的方式缩放，这样选择正则化参数<script type="math/tex">\lambda</script>会更容易，其中m是数据集的大小，这样及时训练集的规模变大，正则化参数<script type="math/tex">\lambda</script>可能还可以使用。参数b在实践中产生的影响很小，所以可以更多的去正则化参数w，而不是b。</p><p>最小化第一项可以让(预测值-真实值)^2^尽可能的小，使算法能更好地拟合数据；最小化第一项可以让参数w尽可能的小，减小过拟合的风险。<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804231054281.png" alt="image-20240804231054281" style="zoom:50%;"></p><p>正则化参数<script type="math/tex">\lambda</script>值体现了相对重要性或相对权衡，即如何取舍上述两个目标。<script type="math/tex">\lambda=0</script>则模型过拟合，<script type="math/tex">\lambda=\infty</script>则模型缺乏对数据的拟合；所以理想中的<script type="math/tex">\lambda</script>值是介于两者之间的，能恰当的平衡第一项和第二项。</p><h4 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h4><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804231701124.png" alt="image-20240804231701124" style="zoom:50%;"><br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804232756152.png" alt="image-20240804232756152" style="zoom:50%;"></p><p><script type="math/tex">w_j(1-\alpha\frac{\lambda}{m})</script>让每次循环w都减少一点点，从而达到正则化的效果。</p><p>代价函数具体的导数推理如下：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804233214830.png" alt="image-20240804233214830" style="zoom:50%;"></p><h4 id="逻辑回归的正则化"><a href="#逻辑回归的正则化" class="headerlink" title="逻辑回归的正则化"></a>逻辑回归的正则化</h4><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804233516282.png" alt="image-20240804233516282" style="zoom:50%;"></p><h2 id="神经网络Neural-Networks"><a href="#神经网络Neural-Networks" class="headerlink" title="神经网络Neural Networks"></a>神经网络Neural Networks</h2><p>应用领域：语音识别speech、计算机视觉images、自然语言处理text(NLP)、……</p><p>激活项(activation)指的是一个神经元向下游的其他神经元发送高输出的数量。</p><p>layer：一层是一组神经元，它让我们输入相同或相似的特征，然后一起输出几个数字。一层可以有一个或多个神经元。</p><p>输出层output layer：最后一个神经元的输出就是整个神经网络预测的输出概率。【逻辑回归】</p><p>输入层input layer、隐藏层hidden layer（作用：自动提取更加好的特征，送入逻辑回归中进行预测）</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240805235712084.png" alt="image-20240805235712084" style="zoom:50%;"><br>有多层的神经网络被称为多层感知器(multilayer perception)</p><h3 id="神经网络工作原理"><a href="#神经网络工作原理" class="headerlink" title="神经网络工作原理"></a>神经网络工作原理</h3><p>函数g是激活函数，本图举例的是S型函数。</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806000738368.png" alt="image-20240806000738368" style="zoom:50%;"><br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806000932626.png" alt="image-20240806000932626" style="zoom:50%;"><br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806001157563.png" alt="image-20240806001157563" style="zoom:50%;"></p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806155510812.png" alt="image-20240806155510812" style="zoom:50%;"></p><p>前向传播forward propagation：为传播神经元的激活值，需要从左到右前进的方向进行计算。</p><h3 id="Tensorflow实践"><a href="#Tensorflow实践" class="headerlink" title="Tensorflow实践"></a>Tensorflow实践</h3><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806160408397.png" alt="image-20240806160408397" style="zoom:50%;"></p><p>密集层（Dense Layer）是深度学习中常用的一种神经网络层，也被称为全连接层（Fully Connected Layer）或线性层（Linear Layer）。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.array([[<span class="number">200.0</span>, <span class="number">17.0</span>]])</span><br><span class="line">layer_1 = Dense(units=<span class="number">3</span>, activation=‘sigmoid’)</span><br><span class="line">a1 = layer_1(x)//a1是一个<span class="number">1</span>*3matrix </span><br><span class="line">// a1: tf.Tensor([[<span class="number">0.2</span> <span class="number">0.7</span> <span class="number">0.3</span>]], shape=(<span class="number">1</span>, <span class="number">3</span>), dtype=float32) 其中Tensor是TF创建的一种数据类型，可以有效地存储和执行矩阵计算，张量比矩阵更具有一般性。</span><br><span class="line">// a1.numpy()=array([[<span class="number">1.4661001</span>, <span class="number">1.125196</span> , <span class="number">3.2159438</span>]], dtype=float32)</span><br><span class="line">layer_2 = Dense(units=<span class="number">1</span>, activation=‘sigmoid’)</span><br><span class="line">a2 = layer_2(a1)// tf.Tensor([[<span class="number">0.8</span>]], shape=(<span class="number">1</span>, <span class="number">1</span>), dtype=float32)</span><br><span class="line">// a2.numpy()=array([[<span class="number">0.8</span>]], dtype=float32)</span><br><span class="line"><span class="keyword">if</span> a2 &gt;= <span class="number">0.5</span>:</span><br><span class="line">yhat = <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">yhat = <span class="number">0</span></span><br><span class="line">------------------------------</span><br><span class="line">layer_1 = Dense(units=<span class="number">3</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">layer_2 = Dense(units=<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">model = Sequential([layer_1, layer_2])// 顺序框架</span><br><span class="line">x = np.array([[<span class="number">200.0</span>, <span class="number">17.0</span>],</span><br><span class="line">              [<span class="number">120.0</span>, <span class="number">5.0</span>],</span><br><span class="line">              [<span class="number">425.0</span>, <span class="number">20.0</span>],</span><br><span class="line">              [<span class="number">212.0</span>, <span class="number">18.0</span>]])</span><br><span class="line">y = np.array([<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">model.<span class="built_in">compile</span>(...)// wait more...</span><br><span class="line">model.fit(x,y)</span><br><span class="line">model.predict(x_new)</span><br></pre></td></tr></table></figure><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806160621956.png" alt="image-20240806160621956" style="zoom:50%;"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">// 手写图形分类</span><br><span class="line">x = np.array([[<span class="number">0.0</span>,..<span class="number">.245</span>,..<span class="number">.240</span>..<span class="number">.0</span>]])</span><br><span class="line">layer_1 = Dense(units=<span class="number">25</span>, activation=‘sigmoid’)</span><br><span class="line">a1 = layer_1(x)</span><br><span class="line">layer_2 = Dense(units=<span class="number">15</span>, activation=‘sigmoid’)</span><br><span class="line">a2 = layer_2(a1)</span><br><span class="line">layer_3 = Dense(units=<span class="number">1</span>, activation=‘sigmoid’)</span><br><span class="line">a3 = layer_3(a2)</span><br><span class="line"><span class="keyword">if</span> a3 &gt;= <span class="number">0.5</span>:</span><br><span class="line">yhat = <span class="number">1</span>// 独热变量</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">yhat = <span class="number">0</span></span><br><span class="line">------------------------------</span><br><span class="line">model = Sequential([</span><br><span class="line">    Dense(units=<span class="number">25</span>, activation=<span class="string">&quot;sigmoid&quot;</span>),</span><br><span class="line">    Dense(units=<span class="number">15</span>, activation=<span class="string">&quot;sigmoid&quot;</span>),</span><br><span class="line">    Dense(units=<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)])</span><br><span class="line">model.<span class="built_in">compile</span>(...)</span><br><span class="line">x = np.array([[<span class="number">0.</span>.., <span class="number">245</span>, ..., <span class="number">17</span>],</span><br><span class="line">  [<span class="number">0.</span>.., <span class="number">200</span>, ..., <span class="number">184</span>]])</span><br><span class="line">y = np.array([<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">model.fit(x,y)</span><br><span class="line">model.predict(x_new)</span><br></pre></td></tr></table></figure><p>独热编码(one-hot)：也称独热变量，是分类变量作为二进制向量的表示。<br>eg：性别特征[“女”,”男”]按照N位状态寄存器来对N个状态进行编码的原理（这里只有两个特征，所以N=2），处理后应该是：女=&gt; 10、男=&gt;01；同理，祖国特征[“中国”，”美国，”法国”]（这里N=3）：中国 =&gt; 100、美国 =&gt; 010、法国 =&gt; 001。<br>所以，当一个样本为[“女”,”中国”]的时候，完整的特征数字化的结果为：[1，0，1，0，0]。而我们使用one-hot编码将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，会让特征之间的距离计算更加合理。</p><h4 id="numpy数据表示"><a href="#numpy数据表示" class="headerlink" title="numpy数据表示"></a>numpy数据表示</h4><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806162303550.png" alt="image-20240806162303550" style="zoom:50%;"></p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806162450369.png" alt="image-20240806162450369" style="zoom:50%;"></p><p>二维矩阵常用于Tensorflow，一维向量常用于线性回归、逻辑回归。</p><h3 id="单层上的向前传播forward-prop"><a href="#单层上的向前传播forward-prop" class="headerlink" title="单层上的向前传播forward prop"></a>单层上的向前传播forward prop</h3><ol><li>设置每一个神经元的w，b值。从而得到z值。</li><li>通过sigmoid function（也就是g(z)）来得到预测值a。</li><li>将第一层的每个神经元的预测结果组合为向量a1。</li><li>使用a1作为layer2的input，再来预测a2的值（最终结果）.</li></ol><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806164444177.png" alt="QQ截图20240806164444" style="zoom:50%;"></p><h3 id="前向传播的一般实现"><a href="#前向传播的一般实现" class="headerlink" title="前向传播的一般实现"></a>前向传播的一般实现</h3><p>简化上面的过程，从而实现更通用的forward prop，而不是对每个神经元写代码</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806170400177.png" alt="image-20240806170400177" style="zoom:50%;"></p>]]></content>
      
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
