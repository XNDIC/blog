<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>PreFLMR运行记录</title>
      <link href="/2024/10/22/PreFLMR%E8%BF%90%E8%A1%8C%E8%AE%B0%E5%BD%95/"/>
      <url>/2024/10/22/PreFLMR%E8%BF%90%E8%A1%8C%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="PreFLMR运行记录"><a href="#PreFLMR运行记录" class="headerlink" title="PreFLMR运行记录"></a>PreFLMR运行记录</h1><h3 id="一、Environment"><a href="#一、Environment" class="headerlink" title="一、Environment"></a>一、Environment</h3><h4 id="1-创建虚拟环境"><a href="#1-创建虚拟环境" class="headerlink" title="1.创建虚拟环境"></a>1.创建虚拟环境</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n lmjFLMR python=3.10 -y</span><br><span class="line">conda create -n FLMR python=3.10 -y</span><br></pre></td></tr></table></figure><p>虚拟环境安装失败，出现<code>An unexpected error has occurred. Conda has prepared the above report.</code>报错。</p><p>解决方法【成功】：<a href="https://www.jianshu.com/p/9e76e679d2f8">删除.condarc文件</a></p><h4 id="2-激活虚拟环境lmjFLMR"><a href="#2-激活虚拟环境lmjFLMR" class="headerlink" title="2.激活虚拟环境lmjFLMR"></a>2.激活虚拟环境lmjFLMR</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda activate lmjFLMR</span><br><span class="line">conda activate FLMR</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/weixin_35757704/article/details/120785822">解决conda activate报错IMPORTANT: You may need to close and restart your shell after running ‘conda init‘</a></p><h4 id="3-安装-Pytorch"><a href="#3-安装-Pytorch" class="headerlink" title="3.安装 Pytorch"></a>3.安装 Pytorch</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</span><br></pre></td></tr></table></figure><p>安装失败，出现<code>ERROR: Could not install packages due to an OSError: [Errno 28] No space left on device</code>。</p><p><img src="/2024/10/22/PreFLMR%E8%BF%90%E8%A1%8C%E8%AE%B0%E5%BD%95/安装pytorch报错.png" alt="安装pytorch报错"></p><p>解决方法1【失败-放弃】：使用<code>--no-cache-dir</code>参数忽略缓存，减少存储空间的使用</p><p>解决方法2【失败-保留】：<a href="https://blog.csdn.net/qq_43340256/article/details/131943325">删除虚拟环境，重新创建虚拟环境时使用–prefix参数，指定虚拟环境的位置</a></p><p>解决方法3【失败-放弃】：<a href="https://blog.csdn.net/sueong/article/details/119829696#:~:text=user%2Dsite%5D%E4%B8%8B%E9%9D%A2-,%E6%94%B9%E5%8F%98pip%E7%9A%84%E5%AE%89%E8%A3%85%E8%B7%AF%E5%BE%84,-mkdir%20packages%20%E5%9C%A8">修改pip安装路径</a></p><p><img src="/2024/10/22/PreFLMR%E8%BF%90%E8%A1%8C%E8%AE%B0%E5%BD%95/image-20241022164013954.png" alt="image-20241022164013954"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip3 --default-timeout=100 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</span><br></pre></td></tr></table></figure><p>最终解决方法【成功】：本地部署</p><p>安装过慢：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -i https://pypi.tuna.tsinghua.edu.cn/simple/ some-package</span><br></pre></td></tr></table></figure><h4 id="4-安装-faiss"><a href="#4-安装-faiss" class="headerlink" title="4.安装 faiss"></a>4.安装 faiss</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda install -c pytorch -c nvidia faiss-gpu=1.7.4 mkl=2021 blas=1.0=mkl</span><br><span class="line">python -c &quot;import faiss&quot;// Test if faiss generate error</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">服务器报错：</span><br><span class="line">CondaError: Downloaded bytes did not match Content-Length</span><br><span class="line">  url: https://conda.anaconda.org/nvidia/linux-64/cudatoolkit-11.4.1-h8ab8bb3_9.tar.bz2                                             </span><br><span class="line">  target_path: /root/miniconda3/pkgs/cudatoolkit-11.4.1-h8ab8bb3_9.tar.bz2           Content-Length: 1033928926                                                         downloaded bytes: 304478570</span><br><span class="line">在用conda安装包的时候下载包的长度不够导致安装包不成功。原因一般是在下载的时候速度较慢，导致下载timeout而终止。</span><br></pre></td></tr></table></figure><p>解决方法：<a href="https://blog.csdn.net/feifei3211/article/details/80361227">设置额外的源</a></p><h4 id="5-安装-FLMR"><a href="#5-安装-FLMR" class="headerlink" title="5.安装 FLMR"></a>5.安装 FLMR</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/LinWeizheDragon/FLMR.git</span><br><span class="line">cd FLMR</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><p><img src="/2024/10/22/PreFLMR%E8%BF%90%E8%A1%8C%E8%AE%B0%E5%BD%95/image-20241022225505312.png" alt="image-20241022225505312"></p><p>解决办法【成功】：<a href="https://blog.csdn.net/qq_45756499/article/details/133863690">代码中默认使用gbk，但是需要改为utf-8</a></p><h4 id="6-安装-ColBERT-引擎"><a href="#6-安装-ColBERT-引擎" class="headerlink" title="6.安装 ColBERT 引擎"></a>6.安装 ColBERT 引擎</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd third_party/ColBERT</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><h4 id="7-安装其他依赖项"><a href="#7-安装其他依赖项" class="headerlink" title="7.安装其他依赖项"></a>7.安装其他依赖项</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install ujson gitpython easydict ninja datasets transformers</span><br></pre></td></tr></table></figure><h4 id="修改hugging-face镜像网站"><a href="#修改hugging-face镜像网站" class="headerlink" title="修改hugging face镜像网站"></a>修改hugging face镜像网站</h4><p><code>/root/miniconda3/envs/FLMR/lib/python3.10/site-packages/huggingface_hub/constants.py</code></p><p><a href="https://blog.csdn.net/m0_61474277/article/details/140348032">解决国内无法连接Hugging face，无法下载预训练模型_服务器连接不上huggingface</a></p><p><img src="/2024/10/22/PreFLMR%E8%BF%90%E8%A1%8C%E8%AE%B0%E5%BD%95/image-20241128211414131.png" alt="image-20241128211414131"></p><h3 id="二、运行FLMR-examples-example-use-custom-functions-py"><a href="#二、运行FLMR-examples-example-use-custom-functions-py" class="headerlink" title="二、运行FLMR\examples\example_use_custom_functions.py"></a>二、运行FLMR\examples\example_use_custom_functions.py</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">UserWarning: Attempted to get default timeout for nccl backend, but NCCL support is not compiled</span><br><span class="line">  warnings.warn(</span><br><span class="line">Process Process-2:</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\multiprocessing\process.py&quot;, line 314, in _bootstrap</span><br><span class="line">    self.run()</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\multiprocessing\process.py&quot;, line 108, in run</span><br><span class="line">    self._target(*self._args, **self._kwargs)</span><br><span class="line">  File &quot;C:\PreFLMR\FLMR\third_party\ColBERT\colbert\infra\launcher.py&quot;, line 109, in setup_new_process</span><br><span class="line">    nranks_, distributed_ = distributed.init(rank)</span><br><span class="line">  File &quot;C:\PreFLMR\FLMR\third_party\ColBERT\colbert\utils\distributed.py&quot;, line 27, in init</span><br><span class="line">    torch.distributed.init_process_group(backend=&#x27;nccl&#x27;, init_method=&#x27;env://&#x27;)</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\torch\distributed\c10d_logger.py&quot;, line 83, in wrapper</span><br><span class="line">    return func(*args, **kwargs)</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\torch\distributed\c10d_logger.py&quot;, line 97, in wrapper</span><br><span class="line">    func_return = func(*args, **kwargs)</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\torch\distributed\distributed_c10d.py&quot;, line 1520, in init_process_group</span><br><span class="line">    store, rank, world_size = next(rendezvous_iterator)</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\torch\distributed\rendezvous.py&quot;, line 269, in _env_rendezvous_handler</span><br><span class="line">    store = _create_c10d_store(</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\torch\distributed\rendezvous.py&quot;, line 189, in _create_c10d_store</span><br><span class="line">    return TCPStore(</span><br><span class="line">RuntimeError: use_libuv was requested but PyTorch was build without libuv support</span><br></pre></td></tr></table></figure><p>解决方法【成功】：①<a href="https://blog.csdn.net/ffffflk/article/details/141871383">FLMR\third_party\ColBERT\colbert\utils\distributed.py的27行添加?use_libuv=False</a>。②<a href="https://blog.csdn.net/m0_61787307/article/details/129638108">添加os.environ两行代码，nccl换成gloo</a></p><h3 id="三、Training-with-contrastive-learning对比学习的训练"><a href="#三、Training-with-contrastive-learning对比学习的训练" class="headerlink" title="三、Training with contrastive learning对比学习的训练"></a>三、Training with contrastive learning对比学习的训练</h3><h4 id="运行FLMR-tools-test-py"><a href="#运行FLMR-tools-test-py" class="headerlink" title="运行FLMR\tools\test.py"></a>运行FLMR\tools\test.py</h4><h5 id="警告"><a href="#警告" class="headerlink" title="警告"></a>警告</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\Administrator\.cache\huggingface\hub\models--LinWeizheDragon--PreFLMR_ViT-L. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.</span><br><span class="line">To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development</span><br><span class="line">  warnings.warn(message)</span><br><span class="line">A new version of the following files was downloaded from https://huggingface.co/LinWeizheDragon/PreFLMR_ViT-L:</span><br><span class="line">- tokenization_flmr.py</span><br><span class="line">. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.</span><br><span class="line">A new version of the following files was downloaded from https://huggingface.co/LinWeizheDragon/PreFLMR_ViT-L:</span><br><span class="line">- configuration_flmr.py</span><br><span class="line">. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.</span><br><span class="line">A new version of the following files was downloaded from https://huggingface.co/LinWeizheDragon/PreFLMR_ViT-L:</span><br><span class="line">- tokenization_flmr_fast.py</span><br><span class="line">. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.</span><br><span class="line">A new version of the following files was downloaded from https://huggingface.co/LinWeizheDragon/PreFLMR_ViT-L:</span><br><span class="line">- flmr_utils.py</span><br><span class="line">. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.</span><br><span class="line">A new version of the following files was downloaded from https://huggingface.co/LinWeizheDragon/PreFLMR_ViT-L:</span><br><span class="line">- modeling_flmr.py</span><br><span class="line">- tokenization_flmr_fast.py</span><br><span class="line">- flmr_utils.py</span><br><span class="line">. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.</span><br></pre></td></tr></table></figure><p>解决方法：<a href="https://github.com/huggingface/huggingface_hub/issues/2179">只是一个警告，告诉在你的设置（Windows + 无开发模式）中，由于不支持符号链接，缓存文件时的用户体验会略有下降。可以隐藏此警告，但不会解决根本问题。</a></p><h5 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\urllib3\response.py&quot;, line 748, in _error_catcher</span><br><span class="line">    yield</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\urllib3\response.py&quot;, line 894, in _raw_read</span><br><span class="line">    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)</span><br><span class="line">urllib3.exceptions.IncompleteRead: IncompleteRead(507634548 bytes read, 1665169612 more expected)</span><br><span class="line"></span><br><span class="line">The above exception was the direct cause of the following exception:</span><br><span class="line"></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\requests\models.py&quot;, line 820, in generate</span><br><span class="line">    yield from self.raw.stream(chunk_size, decode_content=True)</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\urllib3\response.py&quot;, line 1060, in stream</span><br><span class="line">    data = self.read(amt=amt, decode_content=decode_content)</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\urllib3\response.py&quot;, line 977, in read</span><br><span class="line">    data = self._raw_read(amt)</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\urllib3\response.py&quot;, line 872, in _raw_read</span><br><span class="line">    with self._error_catcher():</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\contextlib.py&quot;, line 153, in __exit__</span><br><span class="line">    self.gen.throw(typ, value, traceback)</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\urllib3\response.py&quot;, line 772, in _error_catcher</span><br><span class="line">    raise ProtocolError(arg, e) from e</span><br><span class="line">urllib3.exceptions.ProtocolError: (&#x27;Connection broken: IncompleteRead(507634548 bytes read, 1665169612 more expected)&#x27;, IncompleteRead(507634548 bytes read, 1665169612 more expected))</span><br><span class="line"></span><br><span class="line">During handling of the above exception, another exception occurred:</span><br><span class="line"></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;D:\PreFLMR\FLMR\tools\test.py&quot;, line 10, in &lt;module&gt;</span><br><span class="line">    model = AutoModel.from_pretrained(checkpoint_path,</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\transformers\models\auto\auto_factory.py&quot;, line 559, in from_pretrained</span><br><span class="line">    return model_class.from_pretrained(</span><br><span class="line">  File &quot;C:\Users\Administrator\.cache\huggingface\modules\transformers_modules\LinWeizheDragon\PreFLMR_ViT-L\ca45b7c1867a81af057fde0658fd63ae98b5f8ef\modeling_flmr.py&quot;, line 682, in from_pretrained</span><br><span class="line">    obj = super().from_pretrained(name_or_path, **kwargs)</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\transformers\modeling_utils.py&quot;, line 3604, in from_pretrained</span><br><span class="line">    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\transformers\utils\hub.py&quot;, line 403, in cached_file</span><br><span class="line">    resolved_file = hf_hub_download(</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\huggingface_hub\utils\_validators.py&quot;, line 114, in _inner_fn</span><br><span class="line">    return fn(*args, **kwargs)</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\huggingface_hub\file_download.py&quot;, line 862, in hf_hub_download</span><br><span class="line">    return _hf_hub_download_to_cache_dir(</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\huggingface_hub\file_download.py&quot;, line 1011, in _hf_hub_download_to_cache_dir</span><br><span class="line">    _download_to_tmp_and_move(</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\huggingface_hub\file_download.py&quot;, line 1545, in _download_to_tmp_and_move</span><br><span class="line">    http_get(</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\huggingface_hub\file_download.py&quot;, line 454, in http_get</span><br><span class="line">    for chunk in r.iter_content(chunk_size=constants.DOWNLOAD_CHUNK_SIZE):</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\requests\models.py&quot;, line 822, in generate</span><br><span class="line">    raise ChunkedEncodingError(e)</span><br><span class="line">requests.exceptions.ChunkedEncodingError: (&#x27;Connection broken: IncompleteRead(507634548 bytes read, 1665169612 more expected)&#x27;, IncompleteRead(507634548 bytes read, 1665169612 more expected))</span><br><span class="line"></span><br><span class="line">Process finished with exit code 1</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="四、运行example-use-flmr报错记录"><a href="#四、运行example-use-flmr报错记录" class="headerlink" title="四、运行example_use_flmr报错记录"></a>四、运行example_use_flmr报错记录</h3><h4 id="运行命令行"><a href="#运行命令行" class="headerlink" title="运行命令行"></a>运行命令行</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python example_use_flmr.py --use_gpu --run_indexing --index_root_path &quot;.&quot; --index_name test_index --experiment_name test_experiment --indexing_batch_size 64 --image_root_dir /root/autodl-tmp/preFLMR/ok-vqa --dataset_path /root/autodl-tmp/preFLMR/OKVQA_FLMR_preprocessed_data --passage_dataset_path /root/autodl-tmp/preFLMR/OKVQA_FLMR_preprocessed_GoogleSearch_passages --use_split test --nbits 8 --Ks 1 5 10 20 50 100 --checkpoint_path /root/autodl-tmp/preFLMR/checkpoint_path/FLMR --image_processor_name /root/autodl-tmp/preFLMR/clip-vit-base-patch32 --query_batch_size 8 --num_ROIs 9</span><br></pre></td></tr></table></figure><h4 id="F-PreFLMR-FLMR-third-party-ColBERT-colbert-indexing-collection-indexer-py报错"><a href="#F-PreFLMR-FLMR-third-party-ColBERT-colbert-indexing-collection-indexer-py报错" class="headerlink" title="F:\PreFLMR\FLMR\third_party\ColBERT\colbert\indexing\collection_indexer.py报错"></a><code>F:\PreFLMR\FLMR\third_party\ColBERT\colbert\indexing\collection_indexer.py</code>报错</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Process Process-2:</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\multiprocessing\process.py&quot;, line 314, in _bootstrap</span><br><span class="line">    self.run()</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\multiprocessing\process.py&quot;, line 108, in run</span><br><span class="line">    self._target(*self._args, **self._kwargs)</span><br><span class="line">  File &quot;F:\PreFLMR\FLMR\third_party\ColBERT\colbert\infra\launcher.py&quot;, line 115, in setup_new_process</span><br><span class="line">    return_val = callee(config, *args)</span><br><span class="line">  File &quot;F:\PreFLMR\FLMR\third_party\ColBERT\colbert\indexing\collection_indexer.py&quot;, line 32, in encode</span><br><span class="line">    encoder.run(shared_lists)</span><br><span class="line">  File &quot;F:\PreFLMR\FLMR\third_party\ColBERT\colbert\indexing\collection_indexer.py&quot;, line 63, in run</span><br><span class="line">    self.setup()</span><br><span class="line">  File &quot;F:\PreFLMR\FLMR\third_party\ColBERT\colbert\indexing\collection_indexer.py&quot;, line 93, in setup</span><br><span class="line">    avg_doclen_est = self._sample_embeddings(sampled_pids)</span><br><span class="line">  File &quot;F:\PreFLMR\FLMR\third_party\ColBERT\colbert\indexing\collection_indexer.py&quot;, line 131, in _sample_embeddings</span><br><span class="line">    torch.distributed.all_reduce(self.num_sample_embs)</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\torch\distributed\c10d_logger.py&quot;, line 83, in wrapper</span><br><span class="line">    return func(*args, **kwargs)</span><br><span class="line">  File &quot;C:\Users\Administrator\.conda\envs\FLMR\lib\site-packages\torch\distributed\distributed_c10d.py&quot;, line 2506, in all_reduce</span><br><span class="line">    work.wait()</span><br><span class="line">RuntimeError: Inplace update to inference tensor outside InferenceMode is not allowed.You can make a clone to get a normal tensor before doing inplace update.See https://github.com/pytorch/rfcs/pull/17 for more details.</span><br></pre></td></tr></table></figure><p>解决方法：</p><p>（1）克隆：失败</p><p><img src="/2024/10/22/PreFLMR%E8%BF%90%E8%A1%8C%E8%AE%B0%E5%BD%95/image-20241128173553004.png" alt="image-20241128173553004"></p><p>（2）禁止</p><p><img src="/2024/10/22/PreFLMR%E8%BF%90%E8%A1%8C%E8%AE%B0%E5%BD%95/image-20241128193541870.png" alt="image-20241128193541870"></p><h4 id="报错2"><a href="#报错2" class="headerlink" title="报错2"></a>报错2</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/root/miniconda3/envs/FLMR/lib/python3.10/multiprocessing/process.py&quot;, line 314, in _bootstrap</span><br><span class="line">    self.run()</span><br><span class="line">  File &quot;/root/miniconda3/envs/FLMR/lib/python3.10/multiprocessing/process.py&quot;, line 108, in run</span><br><span class="line">    self._target(*self._args, **self._kwargs)</span><br><span class="line">  File &quot;/root/autodl-tmp/preFLMR/FLMR/third_party/ColBERT/colbert/infra/launcher.py&quot;, line 115, in setup_new_process</span><br><span class="line">    return_val = callee(config, *args)</span><br><span class="line">  File &quot;/root/autodl-tmp/preFLMR/FLMR/third_party/ColBERT/colbert/indexing/collection_indexer.py&quot;, line 32, in encode</span><br><span class="line">    encoder.run(shared_lists)</span><br><span class="line">  File &quot;/root/autodl-tmp/preFLMR/FLMR/third_party/ColBERT/colbert/indexing/collection_indexer.py&quot;, line 68, in run</span><br><span class="line">    self.train(shared_lists)</span><br><span class="line">  File &quot;/root/autodl-tmp/preFLMR/FLMR/third_party/ColBERT/colbert/indexing/collection_indexer.py&quot;, line 217, in train</span><br><span class="line">    centroids = self._train_kmeans(sample, shared_lists)</span><br><span class="line">  File &quot;/root/autodl-tmp/preFLMR/FLMR/third_party/ColBERT/colbert/indexing/collection_indexer.py&quot;, line 286, in _train_kmeans</span><br><span class="line">    centroids = compute_faiss_kmeans(*args_)</span><br><span class="line">  File &quot;/root/autodl-tmp/preFLMR/FLMR/third_party/ColBERT/colbert/indexing/collection_indexer.py&quot;, line 461, in compute_faiss_kmeans</span><br><span class="line">    centroids = torch.from_numpy(kmeans.centroids)</span><br><span class="line">TypeError: expected np.ndarray (got numpy.ndarray)</span><br></pre></td></tr></table></figure><p>/root/autodl-tmp/preFLMR/FLMR/third_party/ColBERT/colbert/indexing/collection_indexer.py</p><p><a href="https://blog.csdn.net/YoJayC/article/details/108762763">TypeError: expected np.ndarray (got int)-CSDN博客</a></p><p><img src="/2024/10/22/PreFLMR%E8%BF%90%E8%A1%8C%E8%AE%B0%E5%BD%95/image-20241128212946806.png" alt="image-20241128212946806"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>conda虚拟环境</title>
      <link href="/2024/10/22/conda%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"/>
      <url>/2024/10/22/conda%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/</url>
      
        <content type="html"><![CDATA[<h1 id="conda虚拟环境"><a href="#conda虚拟环境" class="headerlink" title="conda虚拟环境"></a>conda虚拟环境</h1><h3 id="服务器"><a href="#服务器" class="headerlink" title="服务器"></a>服务器</h3><p>conda create python=3.10 -y —prefix /mnt/data/lmj/lmjFLMR </p><p>environment location: /mnt/data/lmj/lmjFLMR</p><hr><h3 id="创建一个名为myenv的虚拟环境，并指定其位置为-home-user-myenv"><a href="#创建一个名为myenv的虚拟环境，并指定其位置为-home-user-myenv" class="headerlink" title="创建一个名为myenv的虚拟环境，并指定其位置为/home/user/myenv"></a>创建一个名为myenv的虚拟环境，并指定其位置为/home/user/myenv</h3><p>conda create —prefix /home/user/myenv</p><h3 id="激活该虚拟环境"><a href="#激活该虚拟环境" class="headerlink" title="激活该虚拟环境"></a>激活该虚拟环境</h3><p>conda activate /home/user/myenv</p><h3 id="删除该虚拟环境"><a href="#删除该虚拟环境" class="headerlink" title="删除该虚拟环境"></a>删除该虚拟环境</h3><p>conda remove —prefix /home/user/myenv —all</p><p>conda remove -n 虚拟环境名称 —all</p><h4 id="关闭conda环境"><a href="#关闭conda环境" class="headerlink" title="关闭conda环境"></a>关闭conda环境</h4><p>conda deactivate</p>]]></content>
      
      
      
        <tags>
            
            <tag> 环境配置 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3DGS</title>
      <link href="/2024/10/16/3DGS/"/>
      <url>/2024/10/16/3DGS/</url>
      
        <content type="html"><![CDATA[<h2 id="3D-Gaussian-Splatting-3DGS"><a href="#3D-Gaussian-Splatting-3DGS" class="headerlink" title="3D Gaussian Splatting(3DGS)"></a>3D Gaussian Splatting(3DGS)</h2><p>学习来源：</p><p>代码地址：</p><p>论文地址：3D Gaussian Splatting for Real-Time Radiance Field Rendering</p><p>基于Splatting和机器学习的三维重建方法</p><p>无深度学习，只有简单的机器学习＋大量的CG知识＋复杂的线性代数，是对GPU的高性能编程</p><h3 id="Splatting"><a href="#Splatting" class="headerlink" title="Splatting"></a>Splatting</h3><h4 id="基本理解"><a href="#基本理解" class="headerlink" title="基本理解"></a>基本理解</h4><p>定义：一种从3D物体渲染到2D平面的体渲染方法</p><p>NeRF中的体渲染是一种被动的方法(Ray-casting)，计算出每个像素点受到发光粒子的影响来生成图片（主角是像素）。Splatting是主动的，计算出每个发光粒子是如何影响像素点（主角是粒子）。</p><h4 id="为什么叫splatting"><a href="#为什么叫splatting" class="headerlink" title="为什么叫splatting"></a>为什么叫splatting</h4><p>Splat：拟声词，啪唧一声</p><p>想象输入是一些雪球，图片是一面砖墙；图片生成的过程就是向墙面扔雪球的过程，每扔一个雪球，墙面上就会留下扩散痕迹（足迹footprint），同时会有啪唧一声，由此得名。这个算法也可以叫抛雪球算法、足迹法，也会被翻译为喷溅。</p><p>所以splatting的核心：选择“雪球”；抛掷雪球：从3D投影到2D，得到足迹；加以合成，最后形成图像</p><h3 id="选择雪球：3D高斯椭球"><a href="#选择雪球：3D高斯椭球" class="headerlink" title="选择雪球：3D高斯椭球"></a>选择雪球：3D高斯椭球</h3><p>有很好的数学性质：</p><ul><li>仿射变换后高斯核仍然闭合</li><li>3D降维到2D后（沿着某一个轴积分），依然为高斯</li></ul><p>定义：椭球高斯$G(x)=\frac1{\sqrt{(2\pi)^k\mid\sum\mid}}e^{-\frac12(x-\mu)^T\sum^{-1}(x-\mu)}$，其中$\sum$表示协方差矩阵，==半正定==，$\mid\sum\mid$是其行列式</p><p><img src="/2024/10/16/3DGS/image-20241016202550441.png" alt="image-20241016202550441" style="zoom: 80%;"></p><h4 id="为什么是椭球"><a href="#为什么是椭球" class="headerlink" title="为什么是椭球"></a>为什么是椭球</h4><p>椭球：$\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1$，即$Ax^2+By^2+Cz^2+2Dxy+2Eyz+2Fyz=1$</p><h5 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h5><p>一维时形状由均值和方差决定，二维时由均值和协方差矩阵决定，高斯分布概率范围是[0,1]</p><p><img src="/2024/10/16/3DGS/image-20241016203623911.png" alt="image-20241016203623911" style="zoom:80%;"></p><p>协方差矩阵：是一个对称矩阵，决定高斯分布的形状；对角线上的元素是xyz轴的方差，反斜对角线上的值为协方差，表示两者的线性相关程度。<br>$\sum=\left[\begin{matrix}\sigma<em>x^2&amp;\sigma</em>{xy}&amp;\sigma<em>{xz}&amp;\ \sigma</em>{yx}&amp;\sigma<em>y^2&amp;\sigma</em>{yz}&amp;\ \sigma<em>{zx}&amp;\sigma</em>{zy}&amp;\sigma_z^2&amp; \end{matrix}\right]$</p><p>$G(x;\mu,\sum)=\frac1{\sqrt{(2\pi)^k\mid\sum\mid}}exp(-\frac12(x-\mu)^T\sum^{-1}(x-\mu))$<br>当$(x-\mu)^T\sum^{-1}(x-\mu)=constant$时，$G(x;\mu,\sum)=constant$<br>一维时$\frac{(x-\mu)^2}{\sigma^2}=constant$<br>二维时$\frac{(x-\mu<em>1)^2}{\sigma_1^2}+\frac{(y-\mu_2)^2}{\sigma_2^2}-\frac{2\sigma</em>{xy}(x-\mu<em>1)(y-\mu_2)}{\sigma_1\sigma_2}=constant$（椭圆）<br>三维时$constant=(x-\mu)^T\sum^{-1}(x-\mu)=\frac{(x-\mu_1)^2}{\sigma_1^2}+\frac{(y-\mu_2)^2}{\sigma_2^2}+\frac{(z-\mu_3)^2}{\sigma_3^2}-\frac{2\sigma</em>{xy}(x-\mu<em>1)(y-\mu_2)}{\sigma_1\sigma_2}-\frac{2\sigma</em>{xz}(x-\mu<em>1)(y-\mu_3)}{\sigma_1\sigma_3}-\frac{2\sigma</em>{yz}(x-\mu_2)(y-\mu_3)}{\sigma_2\sigma_3}$【注：这个等式最上面的x和第二行的x不是一个概念，上面的x是个3x1的(x,y,z)】，即$Ax^2+By^2+Cz^2+2Dxy+2Eyz+2Fyz=1$，是椭球面</p><p>三维下，$G(x;\mu,\sum)=[0,1]$，每一个常数都是一个椭球壳（g为常数的时候是一个面，而高斯的g是从0-1的连续值），就是椭球壳的分布，所以是大椭球壳套小椭球壳，即实心的椭球</p><h5 id="各向同性和各向异性"><a href="#各向同性和各向异性" class="headerlink" title="各向同性和各向异性"></a>各向同性和各向异性</h5><p>各向同性(Isotropic)：在所有方向都具有相同的扩散程度(梯度)，eg: 球。在3d高斯分布下，协方差矩阵是对角矩阵：$\sum=\left[\begin{matrix}\sigma^2&amp;0&amp;0&amp;\ 0&amp;\sigma^2&amp;0&amp;\ 0&amp;0&amp;\sigma^2&amp; \end{matrix}\right]$，xy、yz、xz之间都不相关</p><p>各向异性(Anisotropic)：在不同方向具有不同的扩散程度(梯度)，eg: 椭球。在3d高斯分布下，协方差矩阵是对角矩阵：$\sum=\left[\begin{matrix}\sigma<em>x^2&amp;\sigma</em>{xy}&amp;\sigma<em>{xz}&amp;\ \sigma</em>{yx}&amp;\sigma<em>y^2&amp;\sigma</em>{yz}&amp;\ \sigma<em>{zx}&amp;\sigma</em>{zy}&amp;\sigma_z^2&amp; \end{matrix}\right]$</p><h4 id="协方差矩阵如何控制椭球形状"><a href="#协方差矩阵如何控制椭球形状" class="headerlink" title="协方差矩阵如何控制椭球形状"></a>协方差矩阵如何控制椭球形状</h4><p>高斯分布：$x\sim N(\mu,\sum)$，其中均值为$[\mu<em>1,\mu_2,\mu_3]$（分别代表x、y、z的值），协方差矩阵为$\sum=\left[\begin{matrix}\sigma_x^2&amp;\sigma</em>{xy}&amp;\sigma<em>{xz}&amp;\ \sigma</em>{yx}&amp;\sigma<em>y^2&amp;\sigma</em>{yz}&amp;\ \sigma<em>{zx}&amp;\sigma</em>{zy}&amp;\sigma_z^2&amp; \end{matrix}\right]$</p><p>高斯分布的仿射变化：存在A、b使得$w=Ax+b$，其中w和x是三维向量，代表xyz；w依然是高斯分布，但均值和方差变了，$w\sim N(A\mu+b,A\sum A^T)$</p><p>标准高斯分布：$x\sim N(\vec0,I)$，其中均值为$[0,0,0]$，协方差矩阵为$I=\left[\begin{matrix}\sigma^2&amp;0&amp;0&amp;\ 0&amp;\sigma^2&amp;0&amp;\ 0&amp;0&amp;\sigma^2&amp; \end{matrix}\right]$</p><p>任意高斯可以看作是标准高斯通过仿射变换得到的，即$\sum=A·I·A^T$，3维下即任意椭球可以从球仿射变换得到，所以协方差矩阵控制了椭球的形状</p><p>仿射变换是通过旋转R、平移b、缩放S得到的，故$w=Ax+b$中，$A=RS$（旋转缩放），所以$\sum=A\cdot I\cdot A^T=R\cdot S\cdot I\cdot (R\cdot S)^T=R\cdot S\cdot (S)^T\cdot (R)^T$，所以<strong>协方差矩阵可以用旋转和缩放矩阵来表示</strong></p><p>一直协方差公式，可以通过特征值分解求R和S：cholesky分解。$\sum=Q\lambda Q^T=Q\lambda^{\frac12}\lambda^{\frac12}Q^T$得到$R=Q$，$S=\lambda^{\frac12}$<br>假设$\lambda=\left[\begin{matrix}s_1&amp;&amp;&amp;\ &amp;s_2&amp;&amp;\ &amp;&amp;s_3&amp; \end{matrix}\right]=\left[\begin{matrix}\sqrt{s_1}&amp;&amp;&amp;\ &amp;\sqrt{s_2}&amp;&amp;\ &amp;&amp;\sqrt{s_3}&amp; \end{matrix}\right]\left[\begin{matrix}\sqrt{s_1}&amp;&amp;&amp;\ &amp;\sqrt{s_2}&amp;&amp;\ &amp;&amp;\sqrt{s_3}&amp; \end{matrix}\right]$，所以$\lambda =\lambda^{\frac12}\lambda^{\frac12}$成立</p><h4 id="计算协方差矩阵代码"><a href="#计算协方差矩阵代码" class="headerlink" title="计算协方差矩阵代码"></a>计算协方差矩阵代码</h4><p><img src="/2024/10/16/3DGS/image-20241016212453023.png" alt="image-20241016212453023"></p><p>输入：scale缩放(3×1维，[s1,s2,s3])，rot旋转矩阵(3*3矩阵)，mod是缩放程度，默认为1</p><h3 id="抛雪球：3D到像素"><a href="#抛雪球：3D到像素" class="headerlink" title="抛雪球：3D到像素"></a>抛雪球：3D到像素</h3><p>CG：观测变换，投影变换，视口变换，光栅化<br>可以参考B站《GAMES101-现代计算机图形学入门-闫令琪》</p><h4 id="观测变换"><a href="#观测变换" class="headerlink" title="观测变换"></a>观测变换</h4><p>世界坐标系到相机坐标系的过程，本质是仿射变换<strong>w</strong>=A<strong>x</strong>+b</p><p><img src="/2024/10/16/3DGS/image-20241016213144332.png" alt="image-20241016213144332" style="zoom:80%;"></p><h4 id="投影变换"><a href="#投影变换" class="headerlink" title="投影变换"></a>投影变换</h4><p>相机坐标系到2D空间的过程，即3D到2D</p><p><img src="/2024/10/16/3DGS/image-20241016213526511.png" alt="image-20241016213526511" style="zoom:80%;"></p><p><strong>正交投影</strong>(右图)：与深度无关，没有远小进大的概念<br><img src="/2024/10/16/3DGS/image-20241016213836626.png" alt="image-20241016213836626"><br>立方体[l,r]×[b,t]×[f,n]先平移到原点，缩放至$[-1,1]^3$的正方体<br>有缩放和平移，相当于仿射变换，即$M_{ortho}=\left[\begin{matrix}\frac2{r-l}&amp;0&amp;0&amp;0&amp;\ 0&amp;\frac2{t-b}&amp;0&amp;0&amp;\ 0&amp;0&amp;\frac2{n-f}&amp;0&amp; \ 0&amp;0&amp;0&amp;1&amp; \end{matrix}\right] \left[\begin{matrix}1&amp;0&amp;0&amp;-\frac{r+l}2&amp;\ 0&amp;1&amp;0&amp;-\frac{t+b}2&amp;\ 0&amp;0&amp;1&amp;-\frac{n+f}2&amp; \ 0&amp;0&amp;0&amp;1&amp; \end{matrix}\right]$</p><p><strong>透视投影</strong>(左图)：考虑深度信息，即远小进大<br><img src="/2024/10/16/3DGS/image-20241016214904364.png" alt="image-20241016214904364" style="zoom:80%;"></p><p>先把锥体压成立方体，再正交投影，即$M_{persp\rightarrow ortho}=\left[\begin{matrix}n&amp;0&amp;0&amp;0&amp;\ 0&amp;n&amp;0&amp;0&amp;\ 0&amp;0&amp;n+f&amp;-nf&amp; \ 0&amp;0&amp;1&amp;0&amp; \end{matrix}\right]$，属于非线性变换，即非仿射变化</p><h4 id="视口变换"><a href="#视口变换" class="headerlink" title="视口变换"></a>视口变换</h4><p>投影变换后得到一个[-1,1]范围内的正方体，再将其拉伸至原始[h,w]的图片大小</p><p><img src="/2024/10/16/3DGS/image-20241016215424640.png" alt="image-20241016215424640" style="zoom:80%;"></p><p>与z无关，将一个$[-1,1]^2$的矩阵变换至$[0,w]×[0,h]$</p><p>$M_{viewport}=\left[\begin{matrix}\frac w2&amp;0&amp;0&amp;\frac w2&amp;\ 0&amp;\frac h2&amp;0&amp;\frac h2&amp;\ 0&amp;0&amp;1&amp;0&amp; \ 0&amp;0&amp;0&amp;1&amp; \end{matrix}\right]$</p><h4 id="光栅化"><a href="#光栅化" class="headerlink" title="光栅化"></a>光栅化</h4><p>物体是连续的，而屏幕本质是离散的。通过采样让连续转离散</p><p><img src="/2024/10/16/3DGS/image-20241016215751737.png" alt="image-20241016215751737" style="zoom:80%;"><img src="/2024/10/16/3DGS/image-20241016215910606.png" alt="image-20241016215910606" style="zoom:80%;"></p><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><p>08：27</p><p><img src="/2024/10/16/3DGS/image-20241016220145497.png" alt="image-20241016220145497"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 数字人 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRF</title>
      <link href="/2024/10/16/NERF/"/>
      <url>/2024/10/16/NERF/</url>
      
        <content type="html"><![CDATA[<h1 id="NERF"><a href="#NERF" class="headerlink" title="NERF"></a>NERF</h1><p>学习来源：<a href="https://www.bilibili.com/video/BV1CC411V7oq/?spm_id_from=333.999.0.0&amp;vd_source=2c97cd7eec4f38788b5479fea364bd65">【较真系列】讲人话-NeRF全解（原理+代码+公式）</a></p><h3 id="什么是NeRF"><a href="#什么是NeRF" class="headerlink" title="什么是NeRF?"></a>什么是NeRF?</h3><h5 id="广义理解定义："><a href="#广义理解定义：" class="headerlink" title="广义理解定义："></a>广义理解定义：</h5><ul><li><p>使用神经网络(MLP)来<strong>隐式的</strong>存储3D信息。</p><ul><li>显式的3D信息：有明确x，y，z的值(mesh，voxel，点云…)<ul><li>mesh：明确的xyz矩阵+边的关系</li></ul></li><li>隐式的3D信息：无明确的x/y/z的值，只能输出指定角度的2D图片。</li></ul></li><li><p>训练时使用给定静止场景下的若干张图片</p><p><img src="/2024/10/16/NERF/image-20240924091335377.png" alt="image-20240924091335377" style="zoom:80%;"></p></li></ul><h5 id="推论："><a href="#推论：" class="headerlink" title="推论："></a>推论：</h5><ul><li>模型不具有泛化能力</li><li>一个模型只能存储一个3D信息</li></ul><p><a href="https://arxiv.org/pdf/2003.08934">论文：</a></p><ol><li>模型输入是5D向量→粒子的空间位姿(x, y, z, theta, phi)</li><li>模型输出是4D向量→粒子对应的颜色以及密度(密度，颜色RGB)</li><li>模型是8层的MLP</li></ol><p>模型前处理：图像转5D向量</p><p>模型后处理：4D向量转2D图片</p><h3 id="真实场景及相机模型"><a href="#真实场景及相机模型" class="headerlink" title="真实场景及相机模型"></a>真实场景及相机模型</h3><p>真实场景：有多个光源+不同物体间的折射/反射</p><p><img src="/2024/10/16/NERF/image-20240924094319157.png" alt="image-20240924094319157" style="zoom:80%;"><img src="/2024/10/16/NERF/image-20240924094415221.png" alt="image-20240924094415221" style="zoom:80%;"></p><p>相机模型：连接3d世界与2d图片</p><ul><li>世界坐标系</li><li>相机坐标系</li><li>归一化相机坐标系（物理成像平面CCD）</li><li>像素坐标系（UV坐标）</li></ul><p>补充：<a href="https://blog.csdn.net/MengYa_Dream/article/details/120233806">图像处理：4个坐标系及相关转换</a></p><h4 id="体渲染"><a href="#体渲染" class="headerlink" title="体渲染"></a>体渲染</h4><p>定义：</p><ul><li>属于渲染技术的分支，目的是解决云/烟/果冻等非刚性物体的渲染建模</li><li>将物质抽象成一团飘忽不定的粒子群</li><li>光线在穿过时，是光子在跟粒子发生碰撞的过程。</li></ul><p>光子与粒子发生作用的过程：</p><ul><li>吸收：光子被粒子吸收</li><li>放射：粒子本身发光</li><li>外射光：光泽在冲击后，被弹射</li><li>内射光：其他方向弹射来的粒子</li></ul><h5 id="NeRF假设："><a href="#NeRF假设：" class="headerlink" title="NeRF假设："></a>NeRF假设：</h5><ul><li>物体是一团自发光的粒子，粒子有密度和颜色</li><li>外射光和内射光抵消</li><li>多个自发光的粒子被渲染成指定角度的图片</li></ul><h3 id="NeRF的输入和输出"><a href="#NeRF的输入和输出" class="headerlink" title="NeRF的输入和输出"></a>NeRF的输入和输出</h3><p>模型的输入：将物体进行稀疏表示的<strong>单个粒子的位姿</strong></p><p>模型的输出：<strong>该粒子的密度和颜色</strong></p><p><img src="/2024/10/16/NERF/image-20240924095330940.png" alt="image-20240924095330940" style="zoom: 60%;"></p><p>问题：</p><ul><li>图片呢？</li><li>怎么得到这些粒子？</li><li>多少个粒子?这些粒子怎么批量输入？</li><li>这些粒子是怎么渲染成新的图片的？</li></ul><h3 id="粒子的采集"><a href="#粒子的采集" class="headerlink" title="粒子的采集"></a>粒子的采集</h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>对于空间中的某一个自发光粒子：假设空间坐标(x,y,z)，发射的光线通过相机模型成为图片上的像素坐标(u,v)，粒子颜色即为像素颜色。</p><p>(u,v)与(x,y,z)的公式如下：</p><p><img src="/2024/10/16/NERF/image-20240924095751794.png" alt="image-20240924095751794" style="zoom:70%;"></p><p>反之，对于图片上的某一个像素(u,v)的颜色，可以看作是<strong>沿着某一条射线上的无数个发光点的“和”</strong>。</p><p>利用相机模型，反推射线，那么这个射线表示为：$r(t)=o+td$，$o$为射线原点，$d$为方向，$t$为距离，理论上t是从0到$+∞$；用极坐标表示。</p><p><img src="/2024/10/16/NERF/image-20240924100410260.png" alt="image-20240924100410260"></p><p>如果一张照片是H×W的大小，对于整张图片共有(H,W)条射线。</p><h4 id="光线原理"><a href="#光线原理" class="headerlink" title="光线原理"></a>光线原理</h4><p>由像素点P(u, v)反推射线：</p><ul><li>像素平面→物理成像平面：$(x_n, y_n)=(-(u-\frac{w}{2}),v-\frac{h}{2})$</li><li>物理成像平面→相机坐标系：$(x_c, y_c, z_c)=(x_n, y_n, -f)$，其中f为焦距</li><li>归一化：$(x_c, y_c, z_c)=(\frac{x_c}{f}, \frac{y_c}{f}, -1)$</li><li>相机坐标系→世界坐标系：$(x_w, y_w, z_w)=c2w*(x_c, y_c, z_c)$，其中c2w指camera to world(RT旋转矩阵)</li></ul><p><img src="/2024/10/16/NERF/image-20241010204145148.png" alt="image-20241010204145148"></p><h4 id="光线代码pytorch-nerf"><a href="#光线代码pytorch-nerf" class="headerlink" title="光线代码pytorch_nerf"></a>光线代码pytorch_nerf</h4><p><img src="/2024/10/16/NERF/image-20241010205316190.png" alt="image-20241010205316190"></p><ul><li>输入：h*w，k相机内参，c2w旋转矩阵+平移矩阵</li><li>i,j：像素的表达，eg: h=400,w=400，i,j表示(0,0)(0,1)…</li><li>dirs：directions，归一化的相机坐标</li><li>rays_d：dirs<em>c2w，射线的<em>*方向</em></em>，世界坐标系</li><li>rays_o：世界坐标系的原点，相机坐标原点*c2w</li><li>输出：rays_o,rays_d，射线的表达</li></ul><p>因为图像的射线有h×w个，过多，要进行筛选，所以引入batch_size，一般为1024。因为原点的表示为[x0,y0,z0]，图像上的一点为[x1,y1,z1]，所以距离为[x0-x1,y0-y1,z0-z1]，即三维[1024,3]。采样是random随机采集像素。采样一般有两种，一种是一张图片选1024个，一种是所有图片中选1024个，每个射线*各自c2w以解决不同的位姿。</p><h4 id="粒子采样原理"><a href="#粒子采样原理" class="headerlink" title="粒子采样原理"></a>粒子采样原理</h4><p>对于图片上的某一个像素(u,v)：沿着某一条射线上的无数个发光点的“和”，射线表示为：$r(t)=o+td$，其中o为射线原点，d为方向，t为距离。理论上t从0到+∞。</p><p>但在计算中t是离散的，所以选择t的方法如下：设置near=2,far=6，在near和far之间均匀采样64个点，得到$t_i=U[t_n+\frac{i-1}{N}(t_f-t_n), t_n+\frac{i}{N}(t_f-t_n)]$</p><p>在2-6中均匀采样64个点并加一些噪声，加一些扰动（效果更好，对feature的噪声更加鲁棒）<br>Robustness：承受故障和干扰的能力，一些异常的数据对整体性能的影响不大。</p><h4 id="粒子采样代码"><a href="#粒子采样代码" class="headerlink" title="粒子采样代码"></a>粒子采样代码</h4><p><img src="/2024/10/16/NERF/image-20241010210044743.png" alt="image-20241010210044743"></p><ul><li><p>near和far：size是(1024,1)，near每个数字都是2，far每个数字都是6</p></li><li><p>linespace：从0-1中采样64个</p></li><li><p>一共是1024*64个点，shape是(1024,64,3)</p></li></ul><h3 id="回答关于输入的问题"><a href="#回答关于输入的问题" class="headerlink" title="回答关于输入的问题"></a>回答关于输入的问题</h3><ol><li>图片呢?怎么得到这些粒子?<br>从图片和相机位姿计算射线，从射线上采样粒子</li><li>多少个粒子?这些粒子怎么批量输入?<br>训练时，一张图片取1024个像素，得到1024条射线，每条射线上采样64个粒子，共1024*64个粒子，粒子以batch形式输入模型</li><li>如何从2D的UV变成5D的输入？<br>以batch形式输入，[1024*64,3]，其中3代表[x,y,z]<br>输入是x,y, z, theta, phi，theta和phi在正常3D表示中分别代表了仰角和方位角，在代码里通过rays_d表示</li></ol><h3 id="输出是什么？"><a href="#输出是什么？" class="headerlink" title="输出是什么？"></a>输出是什么？</h3><p>模型的输出：该粒子的密度和颜色</p><p><img src="/2024/10/16/NERF/image-20241010210440705.png" alt="image-20241010210440705"></p><ul><li>pts：1024<em>64</em>3，即xyz</li><li>viewdirs：方向向量rays_d</li><li>network_fn：MLP多层感知器</li></ul><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>8层全连接层，半路再次输入位置坐标，后半路输出密度σ，后半路输入方向视角，最后输出颜色RGB</p><p><img src="/2024/10/16/NERF/image-20241010211442328.png" alt="image-20241010211442328"></p><h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p><img src="/2024/10/16/NERF/image-20241010210917779.png" alt="image-20241010210917779"></p><p>如果只输入3D的xyz和3D的theta,phi，会丢失细节。所以引入位置编码，即加入sin和cos，和transformer一样</p><ul><li>对于空间坐标x，即xyz，对于每一维度都计算sin和cos，L=10即在0-10进行计算，有20个数，三维加在一起是60维</li><li>对于视角坐标d，即rays_d，L=4即在0-4进行计算，有8个数，三维加在一起是24维</li><li>最后加上原始的三个维度，得到xyz是63维，d是27维【初始值是代表原点？】</li></ul><p>L的值不是固定不变的，可能作者觉得粒子的坐标比方向更重要，所以如此取值。因为模型先通过坐标得到了密度的输出后，在加上direction的信息得到颜色（观测方向不应当影响密度，但会改变颜色）</p><h3 id="模型结构代码"><a href="#模型结构代码" class="headerlink" title="模型结构代码"></a>模型结构代码</h3><p><img src="/2024/10/16/NERF/image-20241010211137004.png" alt="image-20241010211137004"></p><p>整体就是全连接层FC的叠加。在第5层时再次输入位置坐标，即256+63=319；在倒数第二层加入了direction的信息，即256+27=283</p><h3 id="Loss的计算"><a href="#Loss的计算" class="headerlink" title="Loss的计算"></a>Loss的计算</h3><p>采用自监督：GT是图片某一像素的RGB，将该像素对应光线上的粒子颜色进行求和</p><p>粒子的颜色和：该像素颜色的预测值</p><p>粒子的颜色和（真实GT的颜色）与像素颜色（预测的颜色）做MSE：$L=\sum_{r∈R}||\hat{C}(r)-C(r)||_2^2$，其中r是射线，R是每个batch的射线(1024条)</p><h3 id="体渲染-1"><a href="#体渲染-1" class="headerlink" title="体渲染"></a>体渲染</h3><p>如何将光线上粒子颜色进行求和？</p><p>T(s)：在s点之前，光线没有被阻碍的概率；如果光线被阻碍，T(s)=0，则无后续颜色</p><p>σ(s)：在s点处，光线碰击粒子(光线被粒子阻碍)的概率密度</p><p>C(s)：在s点处，粒子光出的颜色</p><p>σ(s)和C(s)是模型的输出，各点的颜色和概率密度已知，先求T(s)</p><p>$\hat{C(r)}=\int_0^{+\infty}T(s)\sigma(s)C(s)ds$</p><p>$T(s)=e^{-\int_0^s\sigma(t)dt}$</p><p><img src="/2024/10/16/NERF/image-20241016135927932.png" alt="image-20241016135927932" style="zoom: 80%;"><img src="/2024/10/16/NERF/image-20241016140605711.png" alt="image-20241016140605711" style="zoom:80%;"></p><h4 id="T-s-的推导"><a href="#T-s-的推导" class="headerlink" title="T(s)的推导"></a>T(s)的推导</h4><p>σ表示密度，密度越大，不被阻拦的概率越小，即1-σ(s)ds</p><p>T(0)在原点不被阻障的概率，T(0)=1，lnT(0)=0</p><p>$T(s+ds)=T(s)[1-\sigma(s)ds]=T(s)-T(s)\sigma(s)ds$</p><p>$T(s+ds)-T(s)=-T(s)\sigma(s)ds$</p><p>$dT(s)=-T(s)\sigma(s)ds$</p><p>$ \frac{dT(s)}{T(s)}=-\sigma(s)ds$</p><p>$\int_0^t-\sigma(s)ds=\int_0^t\frac{dT(s)}{T(s)}=\int_0^t\frac{1}{T(s)}dT(s)=\ln T(s)\vert_0^t=\ln T(t)-\ln T(0)=\ln T(t)$</p><p>$T(t)=e^{-\int_0^s\sigma(t)dt}$</p><h4 id="离散积分"><a href="#离散积分" class="headerlink" title="离散积分"></a>离散积分</h4><p>计算机只能处理离散化数据</p><p>将光线[0,s]划分为N个等间距区间[$T<em>n$→$T</em>{n+1}$]，其中n=0,1,2,…,N；间隔长度为$\delta_n$，假设区间内密度$\sigma_n$和颜色$C_n$固定</p><p>$\hat{C(r)}=\sum<em>{i=1}^NT_i(1-e^{-\sigma_i\delta_i})C_i$，where $T_i=e^{-\sum</em>{j=1}^{i-1}\sigma_j\delta_j}$</p><h5 id="离散积分推导"><a href="#离散积分推导" class="headerlink" title="离散积分推导"></a>离散积分推导</h5><p>每个光区贡献光强进行累加$\hat{C}=\sum<em>{n=0}^NI(T_n\rightarrow T</em>{n+1})$，其中s是沿光流方向的光线的长度，I(s)是距离s处的光强度</p><script type="math/tex; mode=display">\begin{align*}I(T_n\rightarrow T_{n+1}) &= \int_{t_n}^{t_{n+1}}T(t)\sigma(n)C(n)dt \\&= \sigma(n)C(n)\int_{t_n}^{t_{n+1}}T(t)dt \\&= \sigma(n)C(n)\int_{t_n}^{t_{n+1}}e^{-\int_0^t\sigma(s)ds}dt\\&= \sigma(n)C(n)\int_{t_n}^{t_{n+1}}e^{-(\int_0^{t_n}\sigma(s)ds+\int_{t_n}^t\sigma(s)ds)}dt\\&= \sigma(n)C(n)\int_{t_n}^{t_{n+1}}e^{-\int_0^{t_n}\sigma(s)ds}e^{-\int_{t_n}^t\sigma(s)ds}dt\\&= \sigma(n)C(n)T(0\rightarrow t_n)\int_{t_n}^{t_{n+1}}e^{-\int_{t_n}^t\sigma(s)ds}dt\\&= \sigma(n)C(n)T(0\rightarrow t_n)\int_{t_n}^{t_{n+1}}e^{-\int_{t_n}^t\sigma_nds}dt\\&= \sigma(n)C(n)T(0\rightarrow t_n)\int_{t_n}^{t_{n+1}}e^{-\sigma_n(t-t_n)}dt【注：\sigma(n)是常数，做积分原函数是x，故得到t-t_n】\\&= \sigma(n)C(n)T(0\rightarrow t_n)[-\frac1{\sigma_n}e^{-\sigma_n(t-t_n)}\vert_{t_n}^{t_{n+1}}]\\&= \sigma(n)C(n)T(0\rightarrow t_n)(-\frac1{\sigma_n}(e^{-\sigma_n\delta_n}-1))【注：e^{-\sigma_n(t-t_n)}\vert_{t_n}^{t_{n+1}} = e^{-\sigma_n(t_{n+1}-t_n)}-e^{-\sigma_n(t_n-t_n)} = e^{-\sigma_n\delta_n}-1】\\&= C(n)T(0\rightarrow t_n)(-e^{-\sigma_n\delta_n}+1)【注：约去\sigma(n)】\\&= C(n)e^{-\sum_{i=0}^{n-1}\sigma_i\delta_i}(-e^{-\sigma_n\delta_n}+1)【注：T(0\rightarrow t_n)=e^{-\int_0^{t_n}\sigma(s)ds}=e^{-\sum_{i=0}^{n-1}\sigma_i\delta_i}】\\\end{align*}</script><p>$\hat{C}=\sum<em>{n=0}^NI(T_n\rightarrow T</em>{n+1})=\sum<em>{n=0}^NC_ne^{-\sum</em>{i=0}^{n-1}\sigma_i\delta_i}(-e^{1-\sigma_n\delta_n})$</p><p>设$\alpha_n=1-e^{-\sigma_n\delta_n}$，即$e^{-\sigma_n\delta_n}=1-a_n$可以看作第n个点的不透明度</p><script type="math/tex; mode=display">\begin{align*}e^{-\sum_{i=0}^{n-1}\sigma_i\delta_i} &= e^{-(\sigma_0\delta_0+\sigma_1\delta_1+...+\sigma_{n-1}\delta_{n-1})}\\&= e^{-\sigma_0\delta_0}e^{-\sigma_1\delta_1}...e^{-\sigma_{n-1}\delta_{n-1}}\\&= (1-a_0)(1-a_1)...(1-a_{n-1})\\故\hat{C}&=\sum_{n=0}^NC_na_n(1-a_0)(1-a_1)...(1-a_{n-1})\\&=C_0a_0+C_1a_1(1-a_0)+C_2a_2(1-a_0)(1-a_1)+...+C_na_n(1-a_0)(1-a_1)...(1-a_{n-1})\end{align*}</script><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><p>delta_n：粒子取样的间隔，相同的数，大致都是0.06</p><p><img src="/2024/10/16/NERF/image-20241016155640007.png" alt="image-20241016155640007"></p><p>一个射线对应一个像素，最后得到1024*3，3指rgb</p><h3 id="回答关于输出的问题"><a href="#回答关于输出的问题" class="headerlink" title="回答关于输出的问题"></a>回答关于输出的问题</h3><p>这些粒子是怎么渲染成新的图片的？<br>分别计算图片中每一个像素的颜色，计算该像素对应的光线和粒子，将这些粒子通过公式累加，得到该像素最终颜色。</p><h3 id="二次采样"><a href="#二次采样" class="headerlink" title="二次采样"></a>二次采样</h3><p>目前有效区域和无效区域(空白区域和遮挡区域)都是均匀采样</p><p>我们希望：有效区域多采样，无效区域少采样或不采样</p><p>解决方法：可以根据概率密度进行再次采样</p><p><img src="/2024/10/16/NERF/image-20241016160028491.png" alt="image-20241016160028491"></p><h4 id="最终模型结构"><a href="#最终模型结构" class="headerlink" title="最终模型结构"></a>最终模型结构</h4><p>两个8层的MLP串联得到最终模型，第一个MLP为粗模型（均匀采样64个点，得到点的概率密度函数），第二个MLP叫细模型（根据一个模型的概率密度函数，二次采样更有价值点，再进行粒子的求和，得到最终的像素颜色值）</p><p>粗模型和细模型结构相同，最后输出为模型2的输出</p><h4 id="逆变换采样"><a href="#逆变换采样" class="headerlink" title="逆变换采样"></a>逆变换采样</h4><p>细模型根据密度进行二次采样的方法：</p><ul><li>根据粗模型的结果，进行逆变换采样。</li><li>对于每条光线，重新采样128个粒子，与之前的64个粒子加在一起，即每条光线采样192个粒子。</li><li>对于每条射线上的粒子颜色前的权重，即$\alpha<em>n(1-\alpha_0)(1-\alpha_1)…(1-\alpha</em>{n-1})$做softmax，即$\hat{w<em>j}=\frac{w_i}{\sum</em>{j=1}^{N_c}w_i}$，此时，新的权重和为1，可看作PDF(概率密度函数)</li><li>生成它的cdf(累积分布函数，概率密度函数的积分)</li><li>invert cdf(反函数)</li><li>用均匀分布drand480生成一个随机数：invert(drand48)=r，得到的r就是符合pdf分布的随机数</li></ul><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><p><img src="/2024/10/16/NERF/image-20241016161034828.png" alt="image-20241016161034828"></p><p>最后输出(192,3)维度的颜色数据</p><h3 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h3><p>输入：400×400条射线上分别采样64个点</p><p>输出：[400×400×192,4]，其中[h×w×192,4]中4指rgb和σ；进行体渲染</p><p><img src="/2024/10/16/NERF/image-20241016161301952.png" alt="image-20241016161301952"></p><h3 id="Loss是怎样计算的"><a href="#Loss是怎样计算的" class="headerlink" title="Loss是怎样计算的?"></a>Loss是怎样计算的?</h3><p><img src="https://upload-images.jianshu.io/upload_images/12824314-33db12476e16fd6a.png?imageMogr2/auto-orient/strip|imageView2/2/w/1142/format/webp" alt="img"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>前处理：</p><ul><li>将图片中的每个像素，通过相机模型找到对应的射线</li><li>在每条射线上进行采样，得到64个粒子</li><li>对batch_size*64个粒子进行位置编码</li><li>位置坐标为63D和方向向量为27D</li></ul><p>模型1：</p><ul><li>8层MLP</li><li>输入为(batch_size,64,63)和(batch_size,64,27)</li><li>输出为(batch_size,64,4)</li></ul><p>后处理1：</p><ul><li>计算模型1的输出，对射线进行二次采样</li><li>每条射线上共采样192个粒子</li></ul><p>模型2：</p><ul><li>8层MLP</li><li>输入为(batch_size,192,63)和(batch_size,192,27)</li><li>输出为(batch_size,192,4)</li></ul><p>后处理2：将模型2输出通过体渲染，转换为像素。</p><p>核心内容：体渲染、位置编码、层级采样</p><p>缺点：很慢，训练/推理都慢；只能表达静态场景；对光照处理的一般；没有泛化能力。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 数字人 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>吴恩达2022机器学习</title>
      <link href="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
      <url>/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="课程来源"><a href="#课程来源" class="headerlink" title="课程来源"></a>课程来源</h2><p><a href="https://www.bilibili.com/video/BV1Zt4y1H78P/?share_source=copy_web&amp;vd_source=de6d7d10ceabb39ea60e61fa8c108937">吴恩达2022机器学习专项课程(一）监督学习 Supervised Learning</a></p><p><a href="https://www.bilibili.com/video/BV1nt4y1h7jc/?share_source=copy_web&amp;vd_source=de6d7d10ceabb39ea60e61fa8c108937">吴恩达2022机器学习专项课程(二）：高级学习算法 Advanced Learning Algorithms</a></p><p><a href="https://www.bilibili.com/video/BV1ja411S7Wq/?share_source=copy_web&amp;vd_source=de6d7d10ceabb39ea60e61fa8c108937">吴恩达2022机器学习专项课程(三）：无监督学习/推荐系统/强化学习 Unsupervised Learning/Recommenders/Reinforcement</a></p><h2 id="分类算法Classification"><a href="#分类算法Classification" class="headerlink" title="分类算法Classification"></a>分类算法Classification</h2><p>二分类问题(binary classification)：结果只有两种可能的类(classes)，或两种可能的类别(categories)</p><p>术语：正样本(positive class)：true/1；负样本(negative class)：false/0</p><p>线性回归会因为添加训练样本而改变预测结论，如下图所示：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804212737004.png" alt="image-20240804212737004" style="zoom:50%;"></p><h3 id="逻辑回归算法Logistic-Regression"><a href="#逻辑回归算法Logistic-Regression" class="headerlink" title="逻辑回归算法Logistic Regression"></a>逻辑回归算法Logistic Regression</h3><p>逻辑回归方程：$f_\vec{w},_b(\vec{x})=g(\vec{w}\cdot\vec{x}+b)=\frac{1}{1+e^{-(\vec{w}\cdot\vec{x}+b)}}$</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804213348751.png" alt="image-20240804213348751" style="zoom:50%;"></p><p>可以把输出看作是在给定输入x的情况下，类别或标签y等于1的概率。</p><p>$f_\vec{w},_b(\vec{x})=P(y=1|\vec{x},\vec{w},b)$：表明在给定输入特征w和b（参数w和b影响计算）的前提下，y=1的概率是多少（条件概率）</p><p>设置一个阈值(threshold) ，超过这个阈值则预测y=1，通常阈值被设置为0.5。</p><p>通过阈值预测结果的原理如下图蓝框内容：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804214826761.png" alt="image-20240804214826761" style="zoom:50%;"></p><h3 id="决策边界Decision-Boundary"><a href="#决策边界Decision-Boundary" class="headerlink" title="决策边界Decision Boundary"></a>决策边界Decision Boundary</h3><p>线性和非线性决策边界示例如下：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804215353490.png" alt="image-20240804215353490" style="zoom:50%;"><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804215430301.png" alt="image-20240804215430301" style="zoom:50%;"></p><p>通过多项式特征，可以得到非常复杂的决策边界。换包话说，逻辑回归可以学会拟合相当复杂的数据。</p><p>如果不使用高阶多项式，那么逻辑回归的决策边界永远是线性的。</p><h3 id="代价函数Cost-Function"><a href="#代价函数Cost-Function" class="headerlink" title="代价函数Cost Function"></a>代价函数Cost Function</h3><p>逻辑回归函数的代价函数是非凸的(non-convex)，这意味着如果想用梯度下降法，因为有很多局部极小值，所以很容易卡在这些地方。<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804220308650.png" alt="image-20240804220308650" style="zoom:50%;"><br>所以我们需要让代价函数再次凸化，保证梯度下降可以收敛道全局最小值。如上图$J(\vec{w},b)$（1/2被挪到了累加公式里）。</p><p>损失函数衡量的是在一个训练样本中的表现，把所有的训练样本的损失加起来得到的代价函数，才能衡量模型在整个训练集上的表现。</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804221342091.png" alt="image-20240804221342091" style="zoom:50%;"><br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804221614563.png" alt="image-20240804221614563" style="zoom:50%;"></p><p>为了便于写代码，可以将损失函数L简化，简化后的损失函数(loss function)和代价函数(cost function)如下所示：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804222203792.png" alt="image-20240804222203792" style="zoom:50%;"></p><p><a href="https://blog.csdn.net/qq_41775769/article/details/113514294">一文彻底读懂【极大似然估计】-CSDN博客</a></p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>==导数结果不懂，前面整理完后回顾==</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804223110637.png" alt="image-20240804223110637" style="zoom:50%;"></p><p><a href="https://www.cnblogs.com/zhongmiaozhimen/p/6155093.html">第三周：逻辑回归代价函数求导过程 - 玄天妙地 - 博客园 (cnblogs.com)</a></p><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>欠拟合underfit、高偏差high bias、泛化generalization、过拟合overfit、高方差high variance<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804224609969.png" alt="image-20240804224609969" style="zoom:50%;"></p><p>解决过拟合的方法：①增加训练样本②使用更少的特征（特征选择feature selection）③正则化regularization</p><h3 id="正则化Regularization"><a href="#正则化Regularization" class="headerlink" title="正则化Regularization"></a>正则化Regularization</h3><p>正则化是尽可能地让算法缩小参数的值，而不是一定要求参数为0。</p><p>当模型参数很多时，我们不清楚哪些参数是最重要的，就会对所有参数进行惩罚，即把所有参数都缩小点。<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804225912759.png" alt="image-20240804225912759" style="zoom:50%;"><br>上式中<script type="math/tex">\lambda</script>表示正则化参数；累加函数前都除以2m，是为了用同样的方式缩放，这样选择正则化参数<script type="math/tex">\lambda</script>会更容易，其中m是数据集的大小，这样及时训练集的规模变大，正则化参数<script type="math/tex">\lambda</script>可能还可以使用。参数b在实践中产生的影响很小，所以可以更多的去正则化参数w，而不是b。</p><p>最小化第一项可以让(预测值-真实值)^2^尽可能的小，使算法能更好地拟合数据；最小化第一项可以让参数w尽可能的小，减小过拟合的风险。<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804231054281.png" alt="image-20240804231054281" style="zoom:50%;"></p><p>正则化参数<script type="math/tex">\lambda</script>值体现了相对重要性或相对权衡，即如何取舍上述两个目标。<script type="math/tex">\lambda=0</script>则模型过拟合，<script type="math/tex">\lambda=\infty</script>则模型缺乏对数据的拟合；所以理想中的<script type="math/tex">\lambda</script>值是介于两者之间的，能恰当的平衡第一项和第二项。</p><h4 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h4><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804231701124.png" alt="image-20240804231701124" style="zoom:50%;"><br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804232756152.png" alt="image-20240804232756152" style="zoom:50%;"></p><p><script type="math/tex">w_j(1-\alpha\frac{\lambda}{m})</script>让每次循环w都减少一点点，从而达到正则化的效果。</p><p>代价函数具体的导数推理如下：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804233214830.png" alt="image-20240804233214830" style="zoom:50%;"></p><h4 id="逻辑回归的正则化"><a href="#逻辑回归的正则化" class="headerlink" title="逻辑回归的正则化"></a>逻辑回归的正则化</h4><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804233516282.png" alt="image-20240804233516282" style="zoom:50%;"></p><h2 id="神经网络Neural-Networks"><a href="#神经网络Neural-Networks" class="headerlink" title="神经网络Neural Networks"></a>神经网络Neural Networks</h2><p>应用领域：语音识别speech、计算机视觉images、自然语言处理text(NLP)、……</p><p>激活项(activation)指的是一个神经元向下游的其他神经元发送高输出的数量。</p><p>layer：一层是一组神经元，它让我们输入相同或相似的特征，然后一起输出几个数字。一层可以有一个或多个神经元。</p><p>输出层output layer：最后一个神经元的输出就是整个神经网络预测的输出概率。【逻辑回归】</p><p>输入层input layer、隐藏层hidden layer（作用：自动提取更加好的特征，送入逻辑回归中进行预测）</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240805235712084.png" alt="image-20240805235712084" style="zoom:50%;"><br>有多层的神经网络被称为多层感知器(multilayer perception)</p><h3 id="神经网络工作原理"><a href="#神经网络工作原理" class="headerlink" title="神经网络工作原理"></a>神经网络工作原理</h3><p>函数g是激活函数，本图举例的是S型函数。</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806000738368.png" alt="image-20240806000738368" style="zoom:50%;"><br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806000932626.png" alt="image-20240806000932626" style="zoom:50%;"><br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806001157563.png" alt="image-20240806001157563" style="zoom:50%;"></p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806155510812.png" alt="image-20240806155510812" style="zoom:50%;"></p><p>前向传播forward propagation：为传播神经元的激活值，需要从左到右前进的方向进行计算。</p><h3 id="Tensorflow实践"><a href="#Tensorflow实践" class="headerlink" title="Tensorflow实践"></a>Tensorflow实践</h3><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806160408397.png" alt="image-20240806160408397" style="zoom:50%;"></p><p>密集层（Dense Layer）是深度学习中常用的一种神经网络层，也被称为全连接层（Fully Connected Layer）或线性层（Linear Layer）。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.array([[<span class="number">200.0</span>, <span class="number">17.0</span>]])</span><br><span class="line">layer_1 = Dense(units=<span class="number">3</span>, activation=‘sigmoid’)</span><br><span class="line">a1 = layer_1(x)//a1是一个<span class="number">1</span>*3matrix </span><br><span class="line">// a1: tf.Tensor([[<span class="number">0.2</span> <span class="number">0.7</span> <span class="number">0.3</span>]], shape=(<span class="number">1</span>, <span class="number">3</span>), dtype=float32) 其中Tensor是TF创建的一种数据类型，可以有效地存储和执行矩阵计算，张量比矩阵更具有一般性。</span><br><span class="line">// a1.numpy()=array([[<span class="number">1.4661001</span>, <span class="number">1.125196</span> , <span class="number">3.2159438</span>]], dtype=float32)</span><br><span class="line">layer_2 = Dense(units=<span class="number">1</span>, activation=‘sigmoid’)</span><br><span class="line">a2 = layer_2(a1)// tf.Tensor([[<span class="number">0.8</span>]], shape=(<span class="number">1</span>, <span class="number">1</span>), dtype=float32)</span><br><span class="line">// a2.numpy()=array([[<span class="number">0.8</span>]], dtype=float32)</span><br><span class="line"><span class="keyword">if</span> a2 &gt;= <span class="number">0.5</span>:</span><br><span class="line">yhat = <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">yhat = <span class="number">0</span></span><br><span class="line">------------------------------</span><br><span class="line">layer_1 = Dense(units=<span class="number">3</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">layer_2 = Dense(units=<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">model = Sequential([layer_1, layer_2])// 顺序框架</span><br><span class="line">x = np.array([[<span class="number">200.0</span>, <span class="number">17.0</span>],</span><br><span class="line">              [<span class="number">120.0</span>, <span class="number">5.0</span>],</span><br><span class="line">              [<span class="number">425.0</span>, <span class="number">20.0</span>],</span><br><span class="line">              [<span class="number">212.0</span>, <span class="number">18.0</span>]])</span><br><span class="line">y = np.array([<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">model.<span class="built_in">compile</span>(...)// wait more...</span><br><span class="line">model.fit(x,y)</span><br><span class="line">model.predict(x_new)</span><br></pre></td></tr></table></figure><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806160621956.png" alt="image-20240806160621956" style="zoom:50%;"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">// 手写图形分类</span><br><span class="line">x = np.array([[<span class="number">0.0</span>,..<span class="number">.245</span>,..<span class="number">.240</span>..<span class="number">.0</span>]])</span><br><span class="line">layer_1 = Dense(units=<span class="number">25</span>, activation=‘sigmoid’)</span><br><span class="line">a1 = layer_1(x)</span><br><span class="line">layer_2 = Dense(units=<span class="number">15</span>, activation=‘sigmoid’)</span><br><span class="line">a2 = layer_2(a1)</span><br><span class="line">layer_3 = Dense(units=<span class="number">1</span>, activation=‘sigmoid’)</span><br><span class="line">a3 = layer_3(a2)</span><br><span class="line"><span class="keyword">if</span> a3 &gt;= <span class="number">0.5</span>:</span><br><span class="line">yhat = <span class="number">1</span>// 独热变量</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">yhat = <span class="number">0</span></span><br><span class="line">------------------------------</span><br><span class="line">model = Sequential([</span><br><span class="line">    Dense(units=<span class="number">25</span>, activation=<span class="string">&quot;sigmoid&quot;</span>),</span><br><span class="line">    Dense(units=<span class="number">15</span>, activation=<span class="string">&quot;sigmoid&quot;</span>),</span><br><span class="line">    Dense(units=<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)])</span><br><span class="line">model.<span class="built_in">compile</span>(...)</span><br><span class="line">x = np.array([[<span class="number">0.</span>.., <span class="number">245</span>, ..., <span class="number">17</span>],</span><br><span class="line">  [<span class="number">0.</span>.., <span class="number">200</span>, ..., <span class="number">184</span>]])</span><br><span class="line">y = np.array([<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">model.fit(x,y)</span><br><span class="line">model.predict(x_new)</span><br></pre></td></tr></table></figure><p>独热编码(one-hot)：也称独热变量，是分类变量作为二进制向量的表示。<br>eg：性别特征[“女”,”男”]按照N位状态寄存器来对N个状态进行编码的原理（这里只有两个特征，所以N=2），处理后应该是：女=&gt; 10、男=&gt;01；同理，祖国特征[“中国”，”美国，”法国”]（这里N=3）：中国 =&gt; 100、美国 =&gt; 010、法国 =&gt; 001。<br>所以，当一个样本为[“女”,”中国”]的时候，完整的特征数字化的结果为：[1，0，1，0，0]。而我们使用one-hot编码将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，会让特征之间的距离计算更加合理。</p><h4 id="numpy数据表示"><a href="#numpy数据表示" class="headerlink" title="numpy数据表示"></a>numpy数据表示</h4><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806162303550.png" alt="image-20240806162303550" style="zoom:50%;"></p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806162450369.png" alt="image-20240806162450369" style="zoom:50%;"></p><p>二维矩阵常用于Tensorflow，一维向量常用于线性回归、逻辑回归。</p><h3 id="单层上的向前传播forward-prop"><a href="#单层上的向前传播forward-prop" class="headerlink" title="单层上的向前传播forward prop"></a>单层上的向前传播forward prop</h3><ol><li>设置每一个神经元的w，b值。从而得到z值。</li><li>通过sigmoid function（也就是g(z)）来得到预测值a。</li><li>将第一层的每个神经元的预测结果组合为向量a1。</li><li>使用a1作为layer2的input，再来预测a2的值（最终结果）.</li></ol><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806164444177.png" alt="QQ截图20240806164444" style="zoom:50%;"></p><h3 id="前向传播的一般实现"><a href="#前向传播的一般实现" class="headerlink" title="前向传播的一般实现"></a>前向传播的一般实现</h3><p>简化上面的过程，从而实现更通用的forward prop，而不是对每个神经元写代码</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806170400177.png" alt="image-20240806170400177" style="zoom:50%;"></p>]]></content>
      
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
