<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>吴恩达2022机器学习</title>
      <link href="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
      <url>/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="课程来源"><a href="#课程来源" class="headerlink" title="课程来源"></a>课程来源</h2><p><a href="https://www.bilibili.com/video/BV1Zt4y1H78P/?share_source=copy_web&amp;vd_source=de6d7d10ceabb39ea60e61fa8c108937">吴恩达2022机器学习专项课程(一）监督学习 Supervised Learning</a></p><p><a href="https://www.bilibili.com/video/BV1nt4y1h7jc/?share_source=copy_web&amp;vd_source=de6d7d10ceabb39ea60e61fa8c108937">吴恩达2022机器学习专项课程(二）：高级学习算法 Advanced Learning Algorithms</a></p><p><a href="https://www.bilibili.com/video/BV1ja411S7Wq/?share_source=copy_web&amp;vd_source=de6d7d10ceabb39ea60e61fa8c108937">吴恩达2022机器学习专项课程(三）：无监督学习/推荐系统/强化学习 Unsupervised Learning/Recommenders/Reinforcement</a></p><h2 id="分类算法Classification"><a href="#分类算法Classification" class="headerlink" title="分类算法Classification"></a>分类算法Classification</h2><p>二分类问题(binary classification)：结果只有两种可能的类(classes)，或两种可能的类别(categories)</p><p>术语：正样本(positive class)：true/1；负样本(negative class)：false/0</p><p>线性回归会因为添加训练样本而改变预测结论，如下图所示：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804212737004.png" alt="image-20240804212737004" style="zoom:50%;"></p><h3 id="逻辑回归算法Logistic-Regression"><a href="#逻辑回归算法Logistic-Regression" class="headerlink" title="逻辑回归算法Logistic Regression"></a>逻辑回归算法Logistic Regression</h3><p>逻辑回归方程：$f_\vec{w},_b(\vec{x})=g(\vec{w}\cdot\vec{x}+b)=\frac{1}{1+e^{-(\vec{w}\cdot\vec{x}+b)}}$</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804213348751.png" alt="image-20240804213348751" style="zoom:50%;"></p><p>可以把输出看作是在给定输入x的情况下，类别或标签y等于1的概率。</p><p>$f_\vec{w},_b(\vec{x})=P(y=1|\vec{x},\vec{w},b)$：表明在给定输入特征w和b（参数w和b影响计算）的前提下，y=1的概率是多少（条件概率）</p><p>设置一个阈值(threshold) ，超过这个阈值则预测y=1，通常阈值被设置为0.5。</p><p>通过阈值预测结果的原理如下图蓝框内容：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804214826761.png" alt="image-20240804214826761" style="zoom:50%;"></p><h3 id="决策边界Decision-Boundary"><a href="#决策边界Decision-Boundary" class="headerlink" title="决策边界Decision Boundary"></a>决策边界Decision Boundary</h3><p>线性和非线性决策边界示例如下：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804215353490.png" alt="image-20240804215353490" style="zoom:50%;"><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804215430301.png" alt="image-20240804215430301" style="zoom:50%;"></p><p>通过多项式特征，可以得到非常复杂的决策边界。换包话说，逻辑回归可以学会拟合相当复杂的数据。</p><p>如果不使用高阶多项式，那么逻辑回归的决策边界永远是线性的。</p><h3 id="代价函数Cost-Function"><a href="#代价函数Cost-Function" class="headerlink" title="代价函数Cost Function"></a>代价函数Cost Function</h3><p>逻辑回归函数的代价函数是非凸的(non-convex)，这意味着如果想用梯度下降法，因为有很多局部极小值，所以很容易卡在这些地方。<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804220308650.png" alt="image-20240804220308650" style="zoom:50%;"><br>所以我们需要让代价函数再次凸化，保证梯度下降可以收敛道全局最小值。如上图$J(\vec{w},b)$（1/2被挪到了累加公式里）。</p><p>损失函数衡量的是在一个训练样本中的表现，把所有的训练样本的损失加起来得到的代价函数，才能衡量模型在整个训练集上的表现。</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804221342091.png" alt="image-20240804221342091" style="zoom:50%;"><br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804221614563.png" alt="image-20240804221614563" style="zoom:50%;"></p><p>为了便于写代码，可以将损失函数L简化，简化后的损失函数(loss function)和代价函数(cost function)如下所示：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804222203792.png" alt="image-20240804222203792" style="zoom:50%;"></p><p><a href="https://blog.csdn.net/qq_41775769/article/details/113514294">一文彻底读懂【极大似然估计】-CSDN博客</a></p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>==导数结果不懂，前面整理完后回顾==</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804223110637.png" alt="image-20240804223110637" style="zoom:50%;"></p><p><a href="https://www.cnblogs.com/zhongmiaozhimen/p/6155093.html">第三周：逻辑回归代价函数求导过程 - 玄天妙地 - 博客园 (cnblogs.com)</a></p><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>欠拟合underfit、高偏差high bias、泛化generalization、过拟合overfit、高方差high variance<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804224609969.png" alt="image-20240804224609969" style="zoom:50%;"></p><p>解决过拟合的方法：①增加训练样本②使用更少的特征（特征选择feature selection）③正则化regularization</p><h3 id="正则化Regularization"><a href="#正则化Regularization" class="headerlink" title="正则化Regularization"></a>正则化Regularization</h3><p>正则化是尽可能地让算法缩小参数的值，而不是一定要求参数为0。</p><p>当模型参数很多时，我们不清楚哪些参数是最重要的，就会对所有参数进行惩罚，即把所有参数都缩小点。<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804225912759.png" alt="image-20240804225912759" style="zoom:50%;"><br>上式中<script type="math/tex">\lambda</script>表示正则化参数；累加函数前都除以2m，是为了用同样的方式缩放，这样选择正则化参数<script type="math/tex">\lambda</script>会更容易，其中m是数据集的大小，这样及时训练集的规模变大，正则化参数<script type="math/tex">\lambda</script>可能还可以使用。参数b在实践中产生的影响很小，所以可以更多的去正则化参数w，而不是b。</p><p>最小化第一项可以让(预测值-真实值)^2^尽可能的小，使算法能更好地拟合数据；最小化第一项可以让参数w尽可能的小，减小过拟合的风险。<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804231054281.png" alt="image-20240804231054281" style="zoom:50%;"></p><p>正则化参数<script type="math/tex">\lambda</script>值体现了相对重要性或相对权衡，即如何取舍上述两个目标。<script type="math/tex">\lambda=0</script>则模型过拟合，<script type="math/tex">\lambda=\infty</script>则模型缺乏对数据的拟合；所以理想中的<script type="math/tex">\lambda</script>值是介于两者之间的，能恰当的平衡第一项和第二项。</p><h4 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h4><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804231701124.png" alt="image-20240804231701124" style="zoom:50%;"><br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804232756152.png" alt="image-20240804232756152" style="zoom:50%;"></p><p><script type="math/tex">w_j(1-\alpha\frac{\lambda}{m})</script>让每次循环w都减少一点点，从而达到正则化的效果。</p><p>代价函数具体的导数推理如下：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804233214830.png" alt="image-20240804233214830" style="zoom:50%;"></p><h4 id="逻辑回归的正则化"><a href="#逻辑回归的正则化" class="headerlink" title="逻辑回归的正则化"></a>逻辑回归的正则化</h4><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804233516282.png" alt="image-20240804233516282" style="zoom:50%;"></p><h2 id="神经网络Neural-Networks"><a href="#神经网络Neural-Networks" class="headerlink" title="神经网络Neural Networks"></a>神经网络Neural Networks</h2><p>应用领域：语音识别speech、计算机视觉images、自然语言处理text(NLP)、……</p><p>激活项(activation)指的是一个神经元向下游的其他神经元发送高输出的数量。</p><p>layer：一层是一组神经元，它让我们输入相同或相似的特征，然后一起输出几个数字。一层可以有一个或多个神经元。</p><p>输出层output layer：最后一个神经元的输出就是整个神经网络预测的输出概率。【逻辑回归】</p><p>输入层input layer、隐藏层hidden layer（作用：自动提取更加好的特征，送入逻辑回归中进行预测）</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240805235712084.png" alt="image-20240805235712084" style="zoom:50%;"><br>有多层的神经网络被称为多层感知器(multilayer perception)</p><h3 id="神经网络工作原理"><a href="#神经网络工作原理" class="headerlink" title="神经网络工作原理"></a>神经网络工作原理</h3><p>函数g是激活函数，本图举例的是S型函数。</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806000738368.png" alt="image-20240806000738368" style="zoom:50%;"><br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806000932626.png" alt="image-20240806000932626" style="zoom:50%;"><br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806001157563.png" alt="image-20240806001157563" style="zoom:50%;"></p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806155510812.png" alt="image-20240806155510812" style="zoom:50%;"></p><p>前向传播forward propagation：为传播神经元的激活值，需要从左到右前进的方向进行计算。</p><h3 id="Tensorflow实践"><a href="#Tensorflow实践" class="headerlink" title="Tensorflow实践"></a>Tensorflow实践</h3><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806160408397.png" alt="image-20240806160408397" style="zoom:50%;"></p><p>密集层（Dense Layer）是深度学习中常用的一种神经网络层，也被称为全连接层（Fully Connected Layer）或线性层（Linear Layer）。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.array([[<span class="number">200.0</span>, <span class="number">17.0</span>]])</span><br><span class="line">layer_1 = Dense(units=<span class="number">3</span>, activation=‘sigmoid’)</span><br><span class="line">a1 = layer_1(x)//a1是一个<span class="number">1</span>*3matrix </span><br><span class="line">// a1: tf.Tensor([[<span class="number">0.2</span> <span class="number">0.7</span> <span class="number">0.3</span>]], shape=(<span class="number">1</span>, <span class="number">3</span>), dtype=float32) 其中Tensor是TF创建的一种数据类型，可以有效地存储和执行矩阵计算，张量比矩阵更具有一般性。</span><br><span class="line">// a1.numpy()=array([[<span class="number">1.4661001</span>, <span class="number">1.125196</span> , <span class="number">3.2159438</span>]], dtype=float32)</span><br><span class="line">layer_2 = Dense(units=<span class="number">1</span>, activation=‘sigmoid’)</span><br><span class="line">a2 = layer_2(a1)// tf.Tensor([[<span class="number">0.8</span>]], shape=(<span class="number">1</span>, <span class="number">1</span>), dtype=float32)</span><br><span class="line">// a2.numpy()=array([[<span class="number">0.8</span>]], dtype=float32)</span><br><span class="line"><span class="keyword">if</span> a2 &gt;= <span class="number">0.5</span>:</span><br><span class="line">yhat = <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">yhat = <span class="number">0</span></span><br><span class="line">------------------------------</span><br><span class="line">layer_1 = Dense(units=<span class="number">3</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">layer_2 = Dense(units=<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">model = Sequential([layer_1, layer_2])// 顺序框架</span><br><span class="line">x = np.array([[<span class="number">200.0</span>, <span class="number">17.0</span>],</span><br><span class="line">              [<span class="number">120.0</span>, <span class="number">5.0</span>],</span><br><span class="line">              [<span class="number">425.0</span>, <span class="number">20.0</span>],</span><br><span class="line">              [<span class="number">212.0</span>, <span class="number">18.0</span>]])</span><br><span class="line">y = np.array([<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">model.<span class="built_in">compile</span>(...)// wait more...</span><br><span class="line">model.fit(x,y)</span><br><span class="line">model.predict(x_new)</span><br></pre></td></tr></table></figure><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806160621956.png" alt="image-20240806160621956" style="zoom:50%;"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">// 手写图形分类</span><br><span class="line">x = np.array([[<span class="number">0.0</span>,..<span class="number">.245</span>,..<span class="number">.240</span>..<span class="number">.0</span>]])</span><br><span class="line">layer_1 = Dense(units=<span class="number">25</span>, activation=‘sigmoid’)</span><br><span class="line">a1 = layer_1(x)</span><br><span class="line">layer_2 = Dense(units=<span class="number">15</span>, activation=‘sigmoid’)</span><br><span class="line">a2 = layer_2(a1)</span><br><span class="line">layer_3 = Dense(units=<span class="number">1</span>, activation=‘sigmoid’)</span><br><span class="line">a3 = layer_3(a2)</span><br><span class="line"><span class="keyword">if</span> a3 &gt;= <span class="number">0.5</span>:</span><br><span class="line">yhat = <span class="number">1</span>// 独热变量</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">yhat = <span class="number">0</span></span><br><span class="line">------------------------------</span><br><span class="line">model = Sequential([</span><br><span class="line">    Dense(units=<span class="number">25</span>, activation=<span class="string">&quot;sigmoid&quot;</span>),</span><br><span class="line">    Dense(units=<span class="number">15</span>, activation=<span class="string">&quot;sigmoid&quot;</span>),</span><br><span class="line">    Dense(units=<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)])</span><br><span class="line">model.<span class="built_in">compile</span>(...)</span><br><span class="line">x = np.array([[<span class="number">0.</span>.., <span class="number">245</span>, ..., <span class="number">17</span>],</span><br><span class="line">  [<span class="number">0.</span>.., <span class="number">200</span>, ..., <span class="number">184</span>]])</span><br><span class="line">y = np.array([<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">model.fit(x,y)</span><br><span class="line">model.predict(x_new)</span><br></pre></td></tr></table></figure><p>独热编码(one-hot)：也称独热变量，是分类变量作为二进制向量的表示。<br>eg：性别特征[“女”,”男”]按照N位状态寄存器来对N个状态进行编码的原理（这里只有两个特征，所以N=2），处理后应该是：女=&gt; 10、男=&gt;01；同理，祖国特征[“中国”，”美国，”法国”]（这里N=3）：中国 =&gt; 100、美国 =&gt; 010、法国 =&gt; 001。<br>所以，当一个样本为[“女”,”中国”]的时候，完整的特征数字化的结果为：[1，0，1，0，0]。而我们使用one-hot编码将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，会让特征之间的距离计算更加合理。</p><h4 id="numpy数据表示"><a href="#numpy数据表示" class="headerlink" title="numpy数据表示"></a>numpy数据表示</h4><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806162303550.png" alt="image-20240806162303550" style="zoom:50%;"></p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806162450369.png" alt="image-20240806162450369" style="zoom:50%;"></p><p>二维矩阵常用于Tensorflow，一维向量常用于线性回归、逻辑回归。</p><h3 id="单层上的向前传播forward-prop"><a href="#单层上的向前传播forward-prop" class="headerlink" title="单层上的向前传播forward prop"></a>单层上的向前传播forward prop</h3><ol><li>设置每一个神经元的w，b值。从而得到z值。</li><li>通过sigmoid function（也就是g(z)）来得到预测值a。</li><li>将第一层的每个神经元的预测结果组合为向量a1。</li><li>使用a1作为layer2的input，再来预测a2的值（最终结果）.</li></ol><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806164444177.png" alt="QQ截图20240806164444" style="zoom:50%;"></p><h3 id="前向传播的一般实现"><a href="#前向传播的一般实现" class="headerlink" title="前向传播的一般实现"></a>前向传播的一般实现</h3><p>简化上面的过程，从而实现更通用的forward prop，而不是对每个神经元写代码</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806170400177.png" alt="image-20240806170400177" style="zoom:50%;"></p>]]></content>
      
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
