<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>吴恩达2022机器学习</title>
      <link href="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
      <url>/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="课程来源"><a href="#课程来源" class="headerlink" title="课程来源"></a>课程来源</h2><p><a href="https://www.bilibili.com/video/BV1Zt4y1H78P/?share_source=copy_web&amp;vd_source=de6d7d10ceabb39ea60e61fa8c108937">吴恩达2022机器学习专项课程(一）监督学习 Supervised Learning</a></p><p><a href="https://www.bilibili.com/video/BV1nt4y1h7jc/?share_source=copy_web&amp;vd_source=de6d7d10ceabb39ea60e61fa8c108937">吴恩达2022机器学习专项课程(二）：高级学习算法 Advanced Learning Algorithms</a></p><p><a href="https://www.bilibili.com/video/BV1ja411S7Wq/?share_source=copy_web&amp;vd_source=de6d7d10ceabb39ea60e61fa8c108937">吴恩达2022机器学习专项课程(三）：无监督学习/推荐系统/强化学习 Unsupervised Learning/Recommenders/Reinforcement</a></p><h2 id="分类算法Classification"><a href="#分类算法Classification" class="headerlink" title="分类算法Classification"></a>分类算法Classification</h2><p>二分类问题(binary classification)：结果只有两种可能的类(classes)，或两种可能的类别(categories)</p><p>术语：正样本(positive class)：true/1；负样本(negative class)：false/0</p><p>线性回归会因为添加训练样本而改变预测结论，如下图所示：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804212737004.png" alt="image-20240804212737004" style="zoom:50%;"></p><h3 id="逻辑回归算法Logistic-Regression"><a href="#逻辑回归算法Logistic-Regression" class="headerlink" title="逻辑回归算法Logistic Regression"></a>逻辑回归算法Logistic Regression</h3><p>逻辑回归方程：$f_\vec{w},_b(\vec{x})=g(\vec{w}\cdot\vec{x}+b)=\frac{1}{1+e^{-(\vec{w}\cdot\vec{x}+b)}}$</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804213348751.png" alt="image-20240804213348751" style="zoom:50%;"></p><p>可以把输出看作是在给定输入x的情况下，类别或标签y等于1的概率。</p><p>$f_\vec{w},_b(\vec{x})=P(y=1|\vec{x},\vec{w},b)$：表明在给定输入特征w和b（参数w和b影响计算）的前提下，y=1的概率是多少（条件概率）</p><p>设置一个阈值(threshold) ，超过这个阈值则预测y=1，通常阈值被设置为0.5。</p><p>通过阈值预测结果的原理如下图蓝框内容：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804214826761.png" alt="image-20240804214826761" style="zoom:50%;"></p><h3 id="决策边界Decision-Boundary"><a href="#决策边界Decision-Boundary" class="headerlink" title="决策边界Decision Boundary"></a>决策边界Decision Boundary</h3><p>线性和非线性决策边界示例如下：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804215353490.png" alt="image-20240804215353490" style="zoom:50%;"><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804215430301.png" alt="image-20240804215430301" style="zoom:50%;"></p><p>通过多项式特征，可以得到非常复杂的决策边界。换包话说，逻辑回归可以学会拟合相当复杂的数据。</p><p>如果不使用高阶多项式，那么逻辑回归的决策边界永远是线性的。</p><h3 id="代价函数Cost-Function"><a href="#代价函数Cost-Function" class="headerlink" title="代价函数Cost Function"></a>代价函数Cost Function</h3><p>逻辑回归函数的代价函数是非凸的(non-convex)，这意味着如果想用梯度下降法，因为有很多局部极小值，所以很容易卡在这些地方。<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804220308650.png" alt="image-20240804220308650" style="zoom:50%;"><br>所以我们需要让代价函数再次凸化，保证梯度下降可以收敛道全局最小值。如上图$J(\vec{w},b)$（1/2被挪到了累加公式里）。</p><p>损失函数衡量的是在一个训练样本中的表现，把所有的训练样本的损失加起来得到的代价函数，才能衡量模型在整个训练集上的表现。</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804221342091.png" alt="image-20240804221342091" style="zoom:50%;"><br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804221614563.png" alt="image-20240804221614563" style="zoom:50%;"></p><p>为了便于写代码，可以将损失函数L简化，简化后的损失函数(loss function)和代价函数(cost function)如下所示：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804222203792.png" alt="image-20240804222203792" style="zoom:50%;"></p><p><a href="https://blog.csdn.net/qq_41775769/article/details/113514294">一文彻底读懂【极大似然估计】-CSDN博客</a></p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>==导数结果不懂，前面整理完后回顾==</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804223110637.png" alt="image-20240804223110637" style="zoom:50%;"></p><p><a href="https://www.cnblogs.com/zhongmiaozhimen/p/6155093.html">第三周：逻辑回归代价函数求导过程 - 玄天妙地 - 博客园 (cnblogs.com)</a></p><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>欠拟合underfit、高偏差high bias、泛化generalization、过拟合overfit、高方差high variance<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804224609969.png" alt="image-20240804224609969" style="zoom:50%;"></p><p>解决过拟合的方法：①增加训练样本②使用更少的特征（特征选择feature selection）③正则化regularization</p><h3 id="正则化Regularization"><a href="#正则化Regularization" class="headerlink" title="正则化Regularization"></a>正则化Regularization</h3><p>正则化是尽可能地让算法缩小参数的值，而不是一定要求参数为0。</p><p>当模型参数很多时，我们不清楚哪些参数是最重要的，就会对所有参数进行惩罚，即把所有参数都缩小点。<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804225912759.png" alt="image-20240804225912759" style="zoom:50%;"><br>上式中<script type="math/tex">\lambda</script>表示正则化参数；累加函数前都除以2m，是为了用同样的方式缩放，这样选择正则化参数<script type="math/tex">\lambda</script>会更容易，其中m是数据集的大小，这样及时训练集的规模变大，正则化参数<script type="math/tex">\lambda</script>可能还可以使用。参数b在实践中产生的影响很小，所以可以更多的去正则化参数w，而不是b。</p><p>最小化第一项可以让(预测值-真实值)^2^尽可能的小，使算法能更好地拟合数据；最小化第一项可以让参数w尽可能的小，减小过拟合的风险。<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804231054281.png" alt="image-20240804231054281" style="zoom:50%;"></p><p>正则化参数<script type="math/tex">\lambda</script>值体现了相对重要性或相对权衡，即如何取舍上述两个目标。<script type="math/tex">\lambda=0</script>则模型过拟合，<script type="math/tex">\lambda=\infty</script>则模型缺乏对数据的拟合；所以理想中的<script type="math/tex">\lambda</script>值是介于两者之间的，能恰当的平衡第一项和第二项。</p><h4 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h4><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804231701124.png" alt="image-20240804231701124" style="zoom:50%;"><br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804232756152.png" alt="image-20240804232756152" style="zoom:50%;"></p><p><script type="math/tex">w_j(1-\alpha\frac{\lambda}{m})</script>让每次循环w都减少一点点，从而达到正则化的效果。</p><p>代价函数具体的导数推理如下：<br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804233214830.png" alt="image-20240804233214830" style="zoom:50%;"></p><h4 id="逻辑回归的正则化"><a href="#逻辑回归的正则化" class="headerlink" title="逻辑回归的正则化"></a>逻辑回归的正则化</h4><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240804233516282.png" alt="image-20240804233516282" style="zoom:50%;"></p><h2 id="神经网络Neural-Networks"><a href="#神经网络Neural-Networks" class="headerlink" title="神经网络Neural Networks"></a>神经网络Neural Networks</h2><p>应用领域：语音识别speech、计算机视觉images、自然语言处理text(NLP)、……</p><p>激活项(activation)指的是一个神经元向下游的其他神经元发送高输出的数量。</p><p>layer：一层是一组神经元，它让我们输入相同或相似的特征，然后一起输出几个数字。一层可以有一个或多个神经元。</p><p>输出层output layer：最后一个神经元的输出就是整个神经网络预测的输出概率。【逻辑回归】</p><p>输入层input layer、隐藏层hidden layer（作用：自动提取更加好的特征，送入逻辑回归中进行预测）</p><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240805235712084.png" alt="image-20240805235712084" style="zoom:50%;"><br>有多层的神经网络被称为多层感知器(multilayer perception)</p><h3 id="神经网络工作原理"><a href="#神经网络工作原理" class="headerlink" title="神经网络工作原理"></a>神经网络工作原理</h3><p><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806000738368.png" alt="image-20240806000738368" style="zoom:50%;"><br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806000932626.png" alt="image-20240806000932626" style="zoom:50%;"><br><img src="/2024/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE2022%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20240806001157563.png" alt="image-20240806001157563" style="zoom:50%;"></p>]]></content>
      
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
